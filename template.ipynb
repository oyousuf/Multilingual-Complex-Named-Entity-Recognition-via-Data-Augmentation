{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "needed-package",
   "metadata": {},
   "source": [
    "# LwTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "# LwTR - Label-wise Token Replacement\n",
    "def lwtr(train_file):\n",
    "    \n",
    "    lwtr_file = train_file.copy() #make new list to prevent overwritting original file-list\n",
    "    \n",
    "    #for Label-wise Token Replacement, we need to catalogue each possible label\n",
    "    o_label = [line for line in file if ' _ _ O' in line]\n",
    "    b_prod = [line for line in file if ' _ _ B-PROD' in line]\n",
    "    b_grp = [line for line in file if ' _ _ B-GRP' in line]\n",
    "    b_corp = [line for line in file if ' _ _ B-CORP' in line]\n",
    "    b_cw = [line for line in file if ' _ _ B-CW' in line]\n",
    "    b_per = [line for line in file if ' _ _ B-PER' in line]\n",
    "    b_loc = [line for line in file if ' _ _ B-LOC' in line]\n",
    "    i_prod = [line for line in file if ' _ _ I-PROD' in line]\n",
    "    i_grp = [line for line in file if ' _ _ I-GRP' in line]\n",
    "    i_corp = [line for line in file if ' _ _ I-CORP' in line]\n",
    "    i_cw = [line for line in file if ' _ _ I-CW' in line]\n",
    "    i_per = [line for line in file if ' _ _ I-PER' in line]\n",
    "    i_loc = [line for line in file if ' _ _ I-LOC' in line]\n",
    "\n",
    "    #dictionary of labels\n",
    "    lwtr = {'O': o_label, \n",
    "            'B-PROD': b_prod,\n",
    "            'B-GRP': b_grp,\n",
    "            'B-CORP': b_corp,\n",
    "            'B-CW': b_cw,\n",
    "            'B-PER': b_per,\n",
    "            'B-LOC': b_loc,\n",
    "            'I-PROD': i_prod,\n",
    "            'I-GRP': i_grp,\n",
    "            'I-CORP': i_corp,\n",
    "            'I-CW': i_cw,\n",
    "            'I-PER': i_per,\n",
    "            'I-LOC': i_loc}\n",
    "\n",
    "    for index, line in enumerate(train_file): #traverse through file. index:value\n",
    "        x = random.binomial(n=1, p=0.5, size=1) #randomizer. x will be 0 or 1\n",
    "        #x==1 means successly random / startswith() is to filter out id lines / line.strip() is to filter out '\\n' lines\n",
    "        if x == 1 and not line.startswith('# id ') and line.strip(): \n",
    "            curr_label = line.split(' _ _ ')[1].strip() #label found in current line. will be used as the key for lwtr dict\n",
    "            lwtr_file[index] = random.choice(lwtr[curr_label]) #access lwtr dict to randomly choose replacement token of same label and reassign it to current line in file\n",
    "    \n",
    "    return lwtr_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/path/to/ner/file.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "lwtr_file = lwtr(train_file = file)\n",
    "\n",
    "with open('/path/to/save/train_lwtr.conll', 'w') as file:\n",
    "    for line in lwtr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-uganda",
   "metadata": {},
   "source": [
    "# SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syn_replace(train_file, word_vector):\n",
    "    from numpy import random\n",
    "    from gensim.test.utils import datapath\n",
    "    from gensim.models.fasttext import load_facebook_vectors\n",
    "    \n",
    "    wv = load_facebook_vectors(word_vector) #load language word vector\n",
    "    \n",
    "    sr_file = [] #eventual file to write into .conll \n",
    "    aug_count = 0\n",
    "    no_aug_count = 0\n",
    "    for index, line in enumerate(train_file):\n",
    "        if not line.startswith('# id ') and line.strip(): #filter out '#id' and '\\n'\n",
    "            x = random.binomial(n=1, p=0.5, size=1) #random coin flip, 1 is augment, 0 is no augmentation\n",
    "            if x == 1: #augment by replacing current word with the most similar (synonym) word in word vector\n",
    "                separate = line.split(' _ _ ') #to get a list of just the word with the label\n",
    "                word = separate[0] #isolate the word in the line, without it's label\n",
    "                synonym = wv.most_similar(word)[0][0] #synonym of isolated word. 'most_similar' returns list of most top 10 most similar words, with corresponding percentage likelihoods. we just need the MOST likely ([0]), without the percentage ([0])\n",
    "                separate[0] = synonym #replace original word with its new synonym\n",
    "                sr_file.append(' _ _ '.join(separate)) #join back together in format of 'synonym _ _ label'\n",
    "                \n",
    "                aug_count += 1\n",
    "                \n",
    "            else: #no augmentation, still keep original line\n",
    "                sr_file.append(line)\n",
    "                \n",
    "                no_aug_count += 1\n",
    "        else: #append #id's and \\n\n",
    "            sr_file.append(line)\n",
    "    print(f'{aug_count} instances of synonym replacement.')\n",
    "    print(f'{no_aug_count} instances of no synonym replacement.')\n",
    "    return sr_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/path/to/ner/file.conll') as f:\n",
    "    file = f.readlines()\n",
    "sr_file = syn_replace(train_file = file, word_vector = '/path/to/wiki/vector.bin')\n",
    "\n",
    "with open('/path/to/save/train_sr.conll', 'w') as file:\n",
    "    for line in sr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-lounge",
   "metadata": {},
   "source": [
    "# MR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mr(train_file):\n",
    "    ############## DATA STRUCTURES SETUP ###############\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    mention_rep = defaultdict(list) #create default dictionary of lists\n",
    "    entities = ['PER', 'LOC', 'GRP', 'CORP', 'PROD', 'CW'] #all basic NER labels\n",
    "\n",
    "    begin = False #variable for when you've first encountered and captured the head of a mention (B-)\n",
    "    inside = False #variable for when you're inside the mention (I-). There can be 1+ of these\n",
    "\n",
    "    for entity in entities: #traverse through the entities\n",
    "        for line in train_file: #traverse through each line of the language file\n",
    "            if begin: #check to see if you've already run into the head of the mention\n",
    "                if f'I-{entity}' in line: #if not, then see if the I- is in this line\n",
    "                    temp_list.append(line) #if so, you must've gone to the 'elif' below and created the temp_list varible. Append current I- onto this list\n",
    "                    inside = True #switch to True to alert that you're inside the mention from now on\n",
    "\n",
    "                else: #if there's no I- in this line that means you've 1) got the head, and 2) either gotten all I-'s already or there's no more I-'s, and you're done trying to capture a mention\n",
    "                    if mention_rep[entity]: #check to see if you're already started storing values into this entity (PER, LOC, etc.)\n",
    "                        mention_rep[entity].append(temp_list) #if so, simply append the temp_list onto the existing list\n",
    "                    else: #if not that means this is your first entry for an entity's values. defaultdict(list) can't perform append when this happens...\n",
    "                        mention_rep[entity] = [temp_list] #...so we'll have to store the temp_list as a list to prevent the temp_list's contents from being broken into individual values and stored into the dict's value list\n",
    "                    begin = False #revert begin to False for next loop\n",
    "                    inside = False #revert inside for the same reason\n",
    "\n",
    "            elif f'B-{entity}' in line: #if you haven't already run into the head of the mention (B-), check to see if it's in this current loop/line\n",
    "                temp_list = [] #create a temporary list that you'll use to collect a mention's beginning (B-) and insides (I-). this will also reset the temp_list for subsequent loops \n",
    "                temp_list.append(line) #add the beginning labels (B-) to this list\n",
    "                begin = True #switch to True to alert that you've found the head label\n",
    "\n",
    "    #assert every mention was stored as a list\n",
    "    for i in mention_rep: #traverse the dict's keys\n",
    "        for j in mention_rep[i]: #traverse each value in the value-list of the key\n",
    "            assert type(j) is list #check if every value is a list. this is to ensure consistency later on\n",
    "\n",
    "\n",
    "    ################################################# PERFORM MENTION REPLACEMENT ########################################################\n",
    "    import numpy as np #will need ndarray\n",
    "\n",
    "    mr_file = []\n",
    "    labels = tuple(mention_rep.keys()) #set of labels: PER, LOC, GRP, CORP, PROD, CW\n",
    "\n",
    "    #capture_inside = False\n",
    "    for index, line in enumerate(train_file): #traverse through file by line\n",
    "        if line.strip().endswith(labels): #endswith() is to filter out anything other than mentions\n",
    "            if line.strip().split(' _ _ ')[1].startswith('B'): #only get the beginnings of mentions (B-)\n",
    "                x = random.binomial(n = 1, p = 0.5, size = 1) #randomizer. x will be 0 or 1\n",
    "                if x == 1: #x==1 means successfully random, perform mention replacement\n",
    "                    curr_label = line.split(' _ _ ')[1].strip()[2:] #current label type will be used as key next. '2:' is to remove the 'B-' or 'I-' from beginning of label\n",
    "                    new_mention = np.random.choice(np.array(mention_rep[curr_label], dtype = 'object')) #have to make this ndarray because the values of our mention_rep[key] dictionary is lists of LISTS. it becomes deprecated. same logic applies as LwTR above\n",
    "                    if len(new_mention) == 1: #just insert sole element\n",
    "                        mr_file.append(new_mention[0])\n",
    "                    else: #multi-word mention\n",
    "                        for mention in new_mention: #traverse through new_mention list\n",
    "                            mr_file.append(mention)\n",
    "                else: #B-, but no data augmentation\n",
    "                    mr_file.append(line) #add line (non-replaced mention beginning (B-))\n",
    "                    capture_inside = True #this will enable capturing the inside (I-) of the mention if it appears next, instead of disregarding it\n",
    "            elif capture_inside: #add the inside of mention, because we just had a non-replaced mention beginning (B-), so we must capture all insides of original mention\n",
    "                mr_file.append(line) #append non mention replaced inside (I-)\n",
    "\n",
    "        else: #everything that doesn't have a mention label in it\n",
    "            mr_file.append(line) #append current line\n",
    "            capture_inside = False #switch off the flag to indicate we must capture a mention inside\n",
    "\n",
    "    \n",
    "    return mr_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/path/to/ner/file.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "mr_file = mr(train_file = file)\n",
    "\n",
    "with open('/path/to/save/train_mr.conll', 'w') as file:\n",
    "    for line in mr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-sunrise",
   "metadata": {},
   "source": [
    "# SiS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are as follows\n",
    "# PER : Person\n",
    "# LOC : Location\n",
    "# GRP : Group\n",
    "# CORP : Corporation\n",
    "# PROD : Product\n",
    "# CW: Creative Work\n",
    "# O: out-of-mention label (i.e. unrelated to named entity)\n",
    "\n",
    "# B - beginning of label, I - inside label\n",
    "from numpy import random\n",
    "\n",
    "def shuffle(train_file):\n",
    "    labels = tuple(['-PER\\n', '-LOC\\n', '-GRP\\n', '-CORP\\n', '-PROD\\n', '-CW\\n', ' _ _ O']) #set of labels: PER, LOC, GRP, CORP, PROD, CW, and O\n",
    "    shuffle_file = []\n",
    "    prev_label = None\n",
    "    temp_list = []\n",
    "    shuffle_count = 0\n",
    "    non_shuffle_count = 0\n",
    "    \n",
    "    for index, line in enumerate(train_file):\n",
    "        if not line.startswith('# id ') and line.strip(): #to filter out #id's and \\n\n",
    "            for label in labels:\n",
    "                if label in line:\n",
    "                    curr_label = label\n",
    "                    break\n",
    "                    \n",
    "            if curr_label != prev_label and temp_list: #when you've come to a new label, and not on your first loop\n",
    "                x = random.binomial(n=1, p=0.5, size=1)\n",
    "                if x == 1: #perform shuffle data augmnetation\n",
    "                    temp_label = [line.split(' _ _ ')[1] for line in temp_list]\n",
    "                    random.shuffle(temp_list)\n",
    "                    for index, shuff_line in enumerate(temp_list): #traverse through temp list\n",
    "                        shuffle_file.append(shuff_line.split(' _ _ ')[0] + ' _ _ ' + temp_label[index]) #putting labels in their original label\n",
    "                    temp_list.clear() #reset temp_list\n",
    "                    temp_list.append(line) #begin new temp_list with current new label\n",
    "                    prev_label = curr_label #reassign current label value to prev_label to be used as reference for next loop\n",
    "                    shuffle_count += 1\n",
    "                else: #don't shuffle, just append\n",
    "                    for original_line in temp_list:\n",
    "                        shuffle_file.append(original_line)\n",
    "                    temp_list.clear() #reset temp_list\n",
    "                    temp_list.append(line) #begin new temp_list with current new label\n",
    "                    prev_label = curr_label #reassign current label value to prev_label to be used as reference for next loop\n",
    "                    non_shuffle_count += 1\n",
    "            else: #not on transitionary label. this could mean you're on your first label or another of the same label you've been on\n",
    "                temp_list.append(line) #add current line to temp_list\n",
    "                prev_label = curr_label #reassign current label value to prev_label to be used as reference for next loop\n",
    "        elif line == '\\n': #end of current #id NER, shuffle and dump current temp_list holdings\n",
    "            x = random.binomial(n=1, p=0.5, size=1)\n",
    "            if x == 1: #perform shuffle data augmnetation\n",
    "                temp_label = [line.split(' _ _ ')[1] for line in temp_list] #retain labels in order\n",
    "                random.shuffle(temp_list)\n",
    "                for index, shuff_line in enumerate(temp_list): #traverse through temp list\n",
    "                        shuffle_file.append(shuff_line.split(' _ _ ')[0] + ' _ _ ' + temp_label[index]) #putting labels in their original label\n",
    "                shuffle_file.append(line)\n",
    "                temp_list.clear() #reset temp_list\n",
    "                prev_label = None #reassign current label value to prev_label to be used as reference for next loop\n",
    "                shuffle_count += 1\n",
    "            else: #don't shuffle, just append\n",
    "                for original_line in temp_list:\n",
    "                    shuffle_file.append(original_line)\n",
    "                shuffle_file.append(line)\n",
    "                temp_list.clear() #reset temp_list\n",
    "                prev_label = None #reassig\n",
    "                non_shuffle_count += 1\n",
    "        else:\n",
    "            shuffle_file.append(line) #append #id's and \\n\n",
    "            prev_label = None\n",
    "    print(f'{shuffle_count} instances of shuffling.')\n",
    "    print(f'{non_shuffle_count} instances of not shuffling.')\n",
    "    return shuffle_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/path/to/ner/file.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "sis_file = shuffle(train_file = file)\n",
    "\n",
    "with open('/path/to/save/train_mr.conll', 'w') as file:\n",
    "    for line in sis_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-positive",
   "metadata": {},
   "source": [
    "# Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/path/to/ner/file.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "lwtr_file = lwtr(train_file = file)\n",
    "\n",
    "with open('/path/to/ner/file.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "sr_file = syn_replace(train_file = file, word_vector = '/path/to/wiki/vector.bin')\n",
    "\n",
    "with open('/path/to/ner/file.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "mr_file = mr(train_file = file)\n",
    "\n",
    "with open('/path/to/ner/file.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "sis_file = shuffle(train_file = file)\n",
    "\n",
    "combined_file = lwtr_file + sr_file + mr_file + sis_file\n",
    "with open('/path/to/save/train_combined.conll', 'w') as file:\n",
    "    for line in combined_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-manor",
   "metadata": {},
   "source": [
    "# All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/path/to/ner/file.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "    \n",
    "lwtr_file = lwtr(train_file = file)\n",
    "sr_file = syn_replace(train_file = lwtr_file, word_vector = '/path/to/wiki/vector.bin')\n",
    "mr_file = mr(train_file = sr_file)\n",
    "all_file = shuffle(train_file = mr_file)\n",
    "\n",
    "with open('/path/to/save/train_all.conll', 'w') as file:\n",
    "    for line in all_file:\n",
    "        file.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
