{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coastal-essence",
   "metadata": {},
   "source": [
    "# LwTR - Label-wise Token Replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-preserve",
   "metadata": {},
   "source": [
    "From Adel & Dai:\n",
    "\n",
    "Label-wise token replacement (LwTR): For each token, we use a binomial distribution to randomly\n",
    "decide whether it should be replaced. If yes, we then use a label-wise token distribution, built from the\n",
    "original training set, to randomly select another token with the same label. Thus, we keep the original label sequence unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-oriental",
   "metadata": {},
   "source": [
    "# Bangla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "cardiovascular-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/BN-Bangla/bn_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "grave-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each dataset should have 15,300 training sentences\n",
    "original_count = 0\n",
    "for i in file:\n",
    "    if i.startswith('# id '):\n",
    "        original_count += 1\n",
    "assert original_count == 15300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "favorite-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are as follows\n",
    "# PER : Person\n",
    "# LOC : Location\n",
    "# GRP : Group\n",
    "# CORP : Corporation\n",
    "# PROD : Product\n",
    "# CW: Creative Work\n",
    "# O: out-of-mention label (i.e. unrelated to named entity)\n",
    "\n",
    "# B - beginning of label, I - inside label\n",
    "# mentions could be any combination of B/I-label class, and can either be 1 word or multi-word mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "minus-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "lwtr_file = file.copy() #make new list to prevent overwritting original file-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smooth-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Label-wise Token Replacement, we need to catalogue each possible label\n",
    "o_label = [line for line in file if ' _ _ O' in line]\n",
    "b_prod = [line for line in file if ' _ _ B-PROD' in line]\n",
    "b_grp = [line for line in file if ' _ _ B-GRP' in line]\n",
    "b_corp = [line for line in file if ' _ _ B-CORP' in line]\n",
    "b_cw = [line for line in file if ' _ _ B-CW' in line]\n",
    "b_per = [line for line in file if ' _ _ B-PER' in line]\n",
    "b_loc = [line for line in file if ' _ _ B-LOC' in line]\n",
    "i_prod = [line for line in file if ' _ _ I-PROD' in line]\n",
    "i_grp = [line for line in file if ' _ _ I-GRP' in line]\n",
    "i_corp = [line for line in file if ' _ _ I-CORP' in line]\n",
    "i_cw = [line for line in file if ' _ _ I-CW' in line]\n",
    "i_per = [line for line in file if ' _ _ I-PER' in line]\n",
    "i_loc = [line for line in file if ' _ _ I-LOC' in line]\n",
    "\n",
    "#dictionary of labels\n",
    "lwtr = {'O': o_label, \n",
    "        'B-PROD': b_prod,\n",
    "        'B-GRP': b_grp,\n",
    "        'B-CORP': b_corp,\n",
    "        'B-CW': b_cw,\n",
    "        'B-PER': b_per,\n",
    "        'B-LOC': b_loc,\n",
    "        'I-PROD': i_prod,\n",
    "        'I-GRP': i_grp,\n",
    "        'I-CORP': i_corp,\n",
    "        'I-CW': i_cw,\n",
    "        'I-PER': i_per,\n",
    "        'I-LOC': i_loc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "physical-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "for index, line in enumerate(file): #traverse through file. index:value\n",
    "    x = random.binomial(n=1, p=0.5, size=1) #randomizer. x will be 0 or 1\n",
    "    #x==1 means successly random / startswith() is to filter out id lines / line.strip() is to filter out '\\n' lines\n",
    "    if x == 1 and not line.startswith('# id ') and line.strip(): \n",
    "        curr_label = line.split(' _ _ ')[1].strip() #label found in current line. will be used as the key for lwtr dict\n",
    "        lwtr_file[index] = random.choice(lwtr[curr_label]) #access lwtr dict to randomly choose replacement token of same label and reassign it to current line in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "lwtr_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "supported-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/BN-Bangla/bn_train_lwtr.conll', 'w') as file:\n",
    "    for line in lwtr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-dakota",
   "metadata": {},
   "source": [
    "# German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vertical-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/DE-German/de_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "#each dataset should have 15,300 training sentences\n",
    "count = 0\n",
    "for i in file:\n",
    "    if i.startswith('# id '):\n",
    "        count += 1\n",
    "assert count == 15300\n",
    "\n",
    "lwtr_file = file.copy() #make new list to prevent overwritting original file-list\n",
    "\n",
    "\n",
    "#for Label-wise Token Replacement, we need to catalogue each possible label\n",
    "o_label = [line for line in file if ' _ _ O' in line]\n",
    "b_prod = [line for line in file if ' _ _ B-PROD' in line]\n",
    "b_grp = [line for line in file if ' _ _ B-GRP' in line]\n",
    "b_corp = [line for line in file if ' _ _ B-CORP' in line]\n",
    "b_cw = [line for line in file if ' _ _ B-CW' in line]\n",
    "b_per = [line for line in file if ' _ _ B-PER' in line]\n",
    "b_loc = [line for line in file if ' _ _ B-LOC' in line]\n",
    "i_prod = [line for line in file if ' _ _ I-PROD' in line]\n",
    "i_grp = [line for line in file if ' _ _ I-GRP' in line]\n",
    "i_corp = [line for line in file if ' _ _ I-CORP' in line]\n",
    "i_cw = [line for line in file if ' _ _ I-CW' in line]\n",
    "i_per = [line for line in file if ' _ _ I-PER' in line]\n",
    "i_loc = [line for line in file if ' _ _ I-LOC' in line]\n",
    "\n",
    "#dictionary of labels\n",
    "lwtr = {'O': o_label, \n",
    "        'B-PROD': b_prod,\n",
    "        'B-GRP': b_grp,\n",
    "        'B-CORP': b_corp,\n",
    "        'B-CW': b_cw,\n",
    "        'B-PER': b_per,\n",
    "        'B-LOC': b_loc,\n",
    "        'I-PROD': i_prod,\n",
    "        'I-GRP': i_grp,\n",
    "        'I-CORP': i_corp,\n",
    "        'I-CW': i_cw,\n",
    "        'I-PER': i_per,\n",
    "        'I-LOC': i_loc}\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "for index, line in enumerate(file): #traverse through file. index:value\n",
    "    x = random.binomial(n=1, p=0.5, size=1) #randomizer. x will be 0 or 1\n",
    "    #x==1 means successly random / startswith() is to filter out id lines / line.strip() is to filter out '\\n' lines\n",
    "    if x == 1 and not line.startswith('# id ') and line.strip(): \n",
    "        curr_label = line.split(' _ _ ')[1].strip() #label found in current line. will be used as the key for lwtr dict\n",
    "        lwtr_file[index] = random.choice(lwtr[curr_label]) #access lwtr dict to randomly choose replacement token of same label and reassign it to current line in file\n",
    "\n",
    "lwtr_file\n",
    "\n",
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/DE-German/de_train_lwtr.conll', 'w') as file:\n",
    "    for line in lwtr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-weekend",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tired-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/EN-English/en_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "#each dataset should have 15,300 training sentences\n",
    "count = 0\n",
    "for i in file:\n",
    "    if i.startswith('# id '):\n",
    "        count += 1\n",
    "assert count == 15300\n",
    "\n",
    "lwtr_file = file.copy() #make new list to prevent overwritting original file-list\n",
    "\n",
    "\n",
    "#for Label-wise Token Replacement, we need to catalogue each possible label\n",
    "o_label = [line for line in file if ' _ _ O' in line]\n",
    "b_prod = [line for line in file if ' _ _ B-PROD' in line]\n",
    "b_grp = [line for line in file if ' _ _ B-GRP' in line]\n",
    "b_corp = [line for line in file if ' _ _ B-CORP' in line]\n",
    "b_cw = [line for line in file if ' _ _ B-CW' in line]\n",
    "b_per = [line for line in file if ' _ _ B-PER' in line]\n",
    "b_loc = [line for line in file if ' _ _ B-LOC' in line]\n",
    "i_prod = [line for line in file if ' _ _ I-PROD' in line]\n",
    "i_grp = [line for line in file if ' _ _ I-GRP' in line]\n",
    "i_corp = [line for line in file if ' _ _ I-CORP' in line]\n",
    "i_cw = [line for line in file if ' _ _ I-CW' in line]\n",
    "i_per = [line for line in file if ' _ _ I-PER' in line]\n",
    "i_loc = [line for line in file if ' _ _ I-LOC' in line]\n",
    "\n",
    "#dictionary of labels\n",
    "lwtr = {'O': o_label, \n",
    "        'B-PROD': b_prod,\n",
    "        'B-GRP': b_grp,\n",
    "        'B-CORP': b_corp,\n",
    "        'B-CW': b_cw,\n",
    "        'B-PER': b_per,\n",
    "        'B-LOC': b_loc,\n",
    "        'I-PROD': i_prod,\n",
    "        'I-GRP': i_grp,\n",
    "        'I-CORP': i_corp,\n",
    "        'I-CW': i_cw,\n",
    "        'I-PER': i_per,\n",
    "        'I-LOC': i_loc}\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "for index, line in enumerate(file): #traverse through file. index:value\n",
    "    x = random.binomial(n=1, p=0.5, size=1) #randomizer. x will be 0 or 1\n",
    "    #x==1 means successly random / startswith() is to filter out id lines / line.strip() is to filter out '\\n' lines\n",
    "    if x == 1 and not line.startswith('# id ') and line.strip(): \n",
    "        curr_label = line.split(' _ _ ')[1].strip() #label found in current line. will be used as the key for lwtr dict\n",
    "        lwtr_file[index] = random.choice(lwtr[curr_label]) #access lwtr dict to randomly choose replacement token of same label and reassign it to current line in file\n",
    "\n",
    "lwtr_file\n",
    "\n",
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/EN-English/en_train_lwtr.conll', 'w') as file:\n",
    "    for line in lwtr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-equilibrium",
   "metadata": {},
   "source": [
    "# Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "extreme-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/ES-Spanish/es_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "#each dataset should have 15,300 training sentences\n",
    "count = 0\n",
    "for i in file:\n",
    "    if i.startswith('# id '):\n",
    "        count += 1\n",
    "assert count == 15300\n",
    "\n",
    "lwtr_file = file.copy() #make new list to prevent overwritting original file-list\n",
    "\n",
    "\n",
    "#for Label-wise Token Replacement, we need to catalogue each possible label\n",
    "o_label = [line for line in file if ' _ _ O' in line]\n",
    "b_prod = [line for line in file if ' _ _ B-PROD' in line]\n",
    "b_grp = [line for line in file if ' _ _ B-GRP' in line]\n",
    "b_corp = [line for line in file if ' _ _ B-CORP' in line]\n",
    "b_cw = [line for line in file if ' _ _ B-CW' in line]\n",
    "b_per = [line for line in file if ' _ _ B-PER' in line]\n",
    "b_loc = [line for line in file if ' _ _ B-LOC' in line]\n",
    "i_prod = [line for line in file if ' _ _ I-PROD' in line]\n",
    "i_grp = [line for line in file if ' _ _ I-GRP' in line]\n",
    "i_corp = [line for line in file if ' _ _ I-CORP' in line]\n",
    "i_cw = [line for line in file if ' _ _ I-CW' in line]\n",
    "i_per = [line for line in file if ' _ _ I-PER' in line]\n",
    "i_loc = [line for line in file if ' _ _ I-LOC' in line]\n",
    "\n",
    "#dictionary of labels\n",
    "lwtr = {'O': o_label, \n",
    "        'B-PROD': b_prod,\n",
    "        'B-GRP': b_grp,\n",
    "        'B-CORP': b_corp,\n",
    "        'B-CW': b_cw,\n",
    "        'B-PER': b_per,\n",
    "        'B-LOC': b_loc,\n",
    "        'I-PROD': i_prod,\n",
    "        'I-GRP': i_grp,\n",
    "        'I-CORP': i_corp,\n",
    "        'I-CW': i_cw,\n",
    "        'I-PER': i_per,\n",
    "        'I-LOC': i_loc}\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "for index, line in enumerate(file): #traverse through file. index:value\n",
    "    x = random.binomial(n=1, p=0.5, size=1) #randomizer. x will be 0 or 1\n",
    "    #x==1 means successly random / startswith() is to filter out id lines / line.strip() is to filter out '\\n' lines\n",
    "    if x == 1 and not line.startswith('# id ') and line.strip(): \n",
    "        curr_label = line.split(' _ _ ')[1].strip() #label found in current line. will be used as the key for lwtr dict\n",
    "        lwtr_file[index] = random.choice(lwtr[curr_label]) #access lwtr dict to randomly choose replacement token of same label and reassign it to current line in file\n",
    "\n",
    "lwtr_file\n",
    "\n",
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/ES-Spanish/es_train_lwtr.conll', 'w') as file:\n",
    "    for line in lwtr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-bosnia",
   "metadata": {},
   "source": [
    "# Farsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sufficient-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/FA-Farsi/fa_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "#each dataset should have 15,300 training sentences\n",
    "count = 0\n",
    "for i in file:\n",
    "    if i.startswith('# id '):\n",
    "        count += 1\n",
    "assert count == 15300\n",
    "\n",
    "lwtr_file = file.copy() #make new list to prevent overwritting original file-list\n",
    "\n",
    "\n",
    "#for Label-wise Token Replacement, we need to catalogue each possible label\n",
    "o_label = [line for line in file if ' _ _ O' in line]\n",
    "b_prod = [line for line in file if ' _ _ B-PROD' in line]\n",
    "b_grp = [line for line in file if ' _ _ B-GRP' in line]\n",
    "b_corp = [line for line in file if ' _ _ B-CORP' in line]\n",
    "b_cw = [line for line in file if ' _ _ B-CW' in line]\n",
    "b_per = [line for line in file if ' _ _ B-PER' in line]\n",
    "b_loc = [line for line in file if ' _ _ B-LOC' in line]\n",
    "i_prod = [line for line in file if ' _ _ I-PROD' in line]\n",
    "i_grp = [line for line in file if ' _ _ I-GRP' in line]\n",
    "i_corp = [line for line in file if ' _ _ I-CORP' in line]\n",
    "i_cw = [line for line in file if ' _ _ I-CW' in line]\n",
    "i_per = [line for line in file if ' _ _ I-PER' in line]\n",
    "i_loc = [line for line in file if ' _ _ I-LOC' in line]\n",
    "\n",
    "#dictionary of labels\n",
    "lwtr = {'O': o_label, \n",
    "        'B-PROD': b_prod,\n",
    "        'B-GRP': b_grp,\n",
    "        'B-CORP': b_corp,\n",
    "        'B-CW': b_cw,\n",
    "        'B-PER': b_per,\n",
    "        'B-LOC': b_loc,\n",
    "        'I-PROD': i_prod,\n",
    "        'I-GRP': i_grp,\n",
    "        'I-CORP': i_corp,\n",
    "        'I-CW': i_cw,\n",
    "        'I-PER': i_per,\n",
    "        'I-LOC': i_loc}\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "for index, line in enumerate(file): #traverse through file. index:value\n",
    "    x = random.binomial(n=1, p=0.5, size=1) #randomizer. x will be 0 or 1\n",
    "    #x==1 means successly random / startswith() is to filter out id lines / line.strip() is to filter out '\\n' lines\n",
    "    if x == 1 and not line.startswith('# id ') and line.strip(): \n",
    "        curr_label = line.split(' _ _ ')[1].strip() #label found in current line. will be used as the key for lwtr dict\n",
    "        lwtr_file[index] = random.choice(lwtr[curr_label]) #access lwtr dict to randomly choose replacement token of same label and reassign it to current line in file\n",
    "\n",
    "lwtr_file\n",
    "\n",
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/FA-Farsi/fa_train_lwtr.conll', 'w') as file:\n",
    "    for line in lwtr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-crack",
   "metadata": {},
   "source": [
    "# Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "thick-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/HI-Hindi/hi_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "#each dataset should have 15,300 training sentences\n",
    "count = 0\n",
    "for i in file:\n",
    "    if i.startswith('# id '):\n",
    "        count += 1\n",
    "assert count == 15300\n",
    "\n",
    "lwtr_file = file.copy() #make new list to prevent overwritting original file-list\n",
    "\n",
    "\n",
    "#for Label-wise Token Replacement, we need to catalogue each possible label\n",
    "o_label = [line for line in file if ' _ _ O' in line]\n",
    "b_prod = [line for line in file if ' _ _ B-PROD' in line]\n",
    "b_grp = [line for line in file if ' _ _ B-GRP' in line]\n",
    "b_corp = [line for line in file if ' _ _ B-CORP' in line]\n",
    "b_cw = [line for line in file if ' _ _ B-CW' in line]\n",
    "b_per = [line for line in file if ' _ _ B-PER' in line]\n",
    "b_loc = [line for line in file if ' _ _ B-LOC' in line]\n",
    "i_prod = [line for line in file if ' _ _ I-PROD' in line]\n",
    "i_grp = [line for line in file if ' _ _ I-GRP' in line]\n",
    "i_corp = [line for line in file if ' _ _ I-CORP' in line]\n",
    "i_cw = [line for line in file if ' _ _ I-CW' in line]\n",
    "i_per = [line for line in file if ' _ _ I-PER' in line]\n",
    "i_loc = [line for line in file if ' _ _ I-LOC' in line]\n",
    "\n",
    "#dictionary of labels\n",
    "lwtr = {'O': o_label, \n",
    "        'B-PROD': b_prod,\n",
    "        'B-GRP': b_grp,\n",
    "        'B-CORP': b_corp,\n",
    "        'B-CW': b_cw,\n",
    "        'B-PER': b_per,\n",
    "        'B-LOC': b_loc,\n",
    "        'I-PROD': i_prod,\n",
    "        'I-GRP': i_grp,\n",
    "        'I-CORP': i_corp,\n",
    "        'I-CW': i_cw,\n",
    "        'I-PER': i_per,\n",
    "        'I-LOC': i_loc}\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "for index, line in enumerate(file): #traverse through file. index:value\n",
    "    x = random.binomial(n=1, p=0.5, size=1) #randomizer. x will be 0 or 1\n",
    "    #x==1 means successly random / startswith() is to filter out id lines / line.strip() is to filter out '\\n' lines\n",
    "    if x == 1 and not line.startswith('# id ') and line.strip(): \n",
    "        curr_label = line.split(' _ _ ')[1].strip() #label found in current line. will be used as the key for lwtr dict\n",
    "        lwtr_file[index] = random.choice(lwtr[curr_label]) #access lwtr dict to randomly choose replacement token of same label and reassign it to current line in file\n",
    "\n",
    "lwtr_file\n",
    "\n",
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/HI-Hindi/hi_train_lwtr.conll', 'w') as file:\n",
    "    for line in lwtr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-adrian",
   "metadata": {},
   "source": [
    "# Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "essential-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/KO-Korean/ko_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "#each dataset should have 15,300 training sentences\n",
    "count = 0\n",
    "for i in file:\n",
    "    if i.startswith('# id '):\n",
    "        count += 1\n",
    "assert count == 15300\n",
    "\n",
    "lwtr_file = file.copy() #make new list to prevent overwritting original file-list\n",
    "\n",
    "\n",
    "#for Label-wise Token Replacement, we need to catalogue each possible label\n",
    "o_label = [line for line in file if ' _ _ O' in line]\n",
    "b_prod = [line for line in file if ' _ _ B-PROD' in line]\n",
    "b_grp = [line for line in file if ' _ _ B-GRP' in line]\n",
    "b_corp = [line for line in file if ' _ _ B-CORP' in line]\n",
    "b_cw = [line for line in file if ' _ _ B-CW' in line]\n",
    "b_per = [line for line in file if ' _ _ B-PER' in line]\n",
    "b_loc = [line for line in file if ' _ _ B-LOC' in line]\n",
    "i_prod = [line for line in file if ' _ _ I-PROD' in line]\n",
    "i_grp = [line for line in file if ' _ _ I-GRP' in line]\n",
    "i_corp = [line for line in file if ' _ _ I-CORP' in line]\n",
    "i_cw = [line for line in file if ' _ _ I-CW' in line]\n",
    "i_per = [line for line in file if ' _ _ I-PER' in line]\n",
    "i_loc = [line for line in file if ' _ _ I-LOC' in line]\n",
    "\n",
    "#dictionary of labels\n",
    "lwtr = {'O': o_label, \n",
    "        'B-PROD': b_prod,\n",
    "        'B-GRP': b_grp,\n",
    "        'B-CORP': b_corp,\n",
    "        'B-CW': b_cw,\n",
    "        'B-PER': b_per,\n",
    "        'B-LOC': b_loc,\n",
    "        'I-PROD': i_prod,\n",
    "        'I-GRP': i_grp,\n",
    "        'I-CORP': i_corp,\n",
    "        'I-CW': i_cw,\n",
    "        'I-PER': i_per,\n",
    "        'I-LOC': i_loc}\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "for index, line in enumerate(file): #traverse through file. index:value\n",
    "    x = random.binomial(n=1, p=0.5, size=1) #randomizer. x will be 0 or 1\n",
    "    #x==1 means successly random / startswith() is to filter out id lines / line.strip() is to filter out '\\n' lines\n",
    "    if x == 1 and not line.startswith('# id ') and line.strip(): \n",
    "        curr_label = line.split(' _ _ ')[1].strip() #label found in current line. will be used as the key for lwtr dict\n",
    "        lwtr_file[index] = random.choice(lwtr[curr_label]) #access lwtr dict to randomly choose replacement token of same label and reassign it to current line in file\n",
    "\n",
    "lwtr_file\n",
    "\n",
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/KO-Korean/ko_train_lwtr.conll', 'w') as file:\n",
    "    for line in lwtr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-masters",
   "metadata": {},
   "source": [
    "# Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "weird-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/NL-Dutch/nl_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "#each dataset should have 15,300 training sentences\n",
    "count = 0\n",
    "for i in file:\n",
    "    if i.startswith('# id '):\n",
    "        count += 1\n",
    "assert count == 15300\n",
    "\n",
    "lwtr_file = file.copy() #make new list to prevent overwritting original file-list\n",
    "\n",
    "\n",
    "#for Label-wise Token Replacement, we need to catalogue each possible label\n",
    "o_label = [line for line in file if ' _ _ O' in line]\n",
    "b_prod = [line for line in file if ' _ _ B-PROD' in line]\n",
    "b_grp = [line for line in file if ' _ _ B-GRP' in line]\n",
    "b_corp = [line for line in file if ' _ _ B-CORP' in line]\n",
    "b_cw = [line for line in file if ' _ _ B-CW' in line]\n",
    "b_per = [line for line in file if ' _ _ B-PER' in line]\n",
    "b_loc = [line for line in file if ' _ _ B-LOC' in line]\n",
    "i_prod = [line for line in file if ' _ _ I-PROD' in line]\n",
    "i_grp = [line for line in file if ' _ _ I-GRP' in line]\n",
    "i_corp = [line for line in file if ' _ _ I-CORP' in line]\n",
    "i_cw = [line for line in file if ' _ _ I-CW' in line]\n",
    "i_per = [line for line in file if ' _ _ I-PER' in line]\n",
    "i_loc = [line for line in file if ' _ _ I-LOC' in line]\n",
    "\n",
    "#dictionary of labels\n",
    "lwtr = {'O': o_label, \n",
    "        'B-PROD': b_prod,\n",
    "        'B-GRP': b_grp,\n",
    "        'B-CORP': b_corp,\n",
    "        'B-CW': b_cw,\n",
    "        'B-PER': b_per,\n",
    "        'B-LOC': b_loc,\n",
    "        'I-PROD': i_prod,\n",
    "        'I-GRP': i_grp,\n",
    "        'I-CORP': i_corp,\n",
    "        'I-CW': i_cw,\n",
    "        'I-PER': i_per,\n",
    "        'I-LOC': i_loc}\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "for index, line in enumerate(file): #traverse through file. index:value\n",
    "    x = random.binomial(n=1, p=0.5, size=1) #randomizer. x will be 0 or 1\n",
    "    #x==1 means successly random / startswith() is to filter out id lines / line.strip() is to filter out '\\n' lines\n",
    "    if x == 1 and not line.startswith('# id ') and line.strip(): \n",
    "        curr_label = line.split(' _ _ ')[1].strip() #label found in current line. will be used as the key for lwtr dict\n",
    "        lwtr_file[index] = random.choice(lwtr[curr_label]) #access lwtr dict to randomly choose replacement token of same label and reassign it to current line in file\n",
    "\n",
    "lwtr_file\n",
    "\n",
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/NL-Dutch/nl_train_lwtr.conll', 'w') as file:\n",
    "    for line in lwtr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-thickness",
   "metadata": {},
   "source": [
    "# Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "guided-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/RU-Russian/ru_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "#each dataset should have 15,300 training sentences\n",
    "count = 0\n",
    "for i in file:\n",
    "    if i.startswith('# id '):\n",
    "        count += 1\n",
    "assert count == 15300\n",
    "\n",
    "lwtr_file = file.copy() #make new list to prevent overwritting original file-list\n",
    "\n",
    "\n",
    "#for Label-wise Token Replacement, we need to catalogue each possible label\n",
    "o_label = [line for line in file if ' _ _ O' in line]\n",
    "b_prod = [line for line in file if ' _ _ B-PROD' in line]\n",
    "b_grp = [line for line in file if ' _ _ B-GRP' in line]\n",
    "b_corp = [line for line in file if ' _ _ B-CORP' in line]\n",
    "b_cw = [line for line in file if ' _ _ B-CW' in line]\n",
    "b_per = [line for line in file if ' _ _ B-PER' in line]\n",
    "b_loc = [line for line in file if ' _ _ B-LOC' in line]\n",
    "i_prod = [line for line in file if ' _ _ I-PROD' in line]\n",
    "i_grp = [line for line in file if ' _ _ I-GRP' in line]\n",
    "i_corp = [line for line in file if ' _ _ I-CORP' in line]\n",
    "i_cw = [line for line in file if ' _ _ I-CW' in line]\n",
    "i_per = [line for line in file if ' _ _ I-PER' in line]\n",
    "i_loc = [line for line in file if ' _ _ I-LOC' in line]\n",
    "\n",
    "#dictionary of labels\n",
    "lwtr = {'O': o_label, \n",
    "        'B-PROD': b_prod,\n",
    "        'B-GRP': b_grp,\n",
    "        'B-CORP': b_corp,\n",
    "        'B-CW': b_cw,\n",
    "        'B-PER': b_per,\n",
    "        'B-LOC': b_loc,\n",
    "        'I-PROD': i_prod,\n",
    "        'I-GRP': i_grp,\n",
    "        'I-CORP': i_corp,\n",
    "        'I-CW': i_cw,\n",
    "        'I-PER': i_per,\n",
    "        'I-LOC': i_loc}\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "for index, line in enumerate(file): #traverse through file. index:value\n",
    "    x = random.binomial(n=1, p=0.5, size=1) #randomizer. x will be 0 or 1\n",
    "    #x==1 means successly random / startswith() is to filter out id lines / line.strip() is to filter out '\\n' lines\n",
    "    if x == 1 and not line.startswith('# id ') and line.strip(): \n",
    "        curr_label = line.split(' _ _ ')[1].strip() #label found in current line. will be used as the key for lwtr dict\n",
    "        lwtr_file[index] = random.choice(lwtr[curr_label]) #access lwtr dict to randomly choose replacement token of same label and reassign it to current line in file\n",
    "\n",
    "lwtr_file\n",
    "\n",
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/RU-Russian/ru_train_lwtr.conll', 'w') as file:\n",
    "    for line in lwtr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-trigger",
   "metadata": {},
   "source": [
    "# Turkish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "innovative-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/TR-Turkish/tr_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "#each dataset should have 15,300 training sentences\n",
    "count = 0\n",
    "for i in file:\n",
    "    if i.startswith('# id '):\n",
    "        count += 1\n",
    "assert count == 15300\n",
    "\n",
    "lwtr_file = file.copy() #make new list to prevent overwritting original file-list\n",
    "\n",
    "\n",
    "#for Label-wise Token Replacement, we need to catalogue each possible label\n",
    "o_label = [line for line in file if ' _ _ O' in line]\n",
    "b_prod = [line for line in file if ' _ _ B-PROD' in line]\n",
    "b_grp = [line for line in file if ' _ _ B-GRP' in line]\n",
    "b_corp = [line for line in file if ' _ _ B-CORP' in line]\n",
    "b_cw = [line for line in file if ' _ _ B-CW' in line]\n",
    "b_per = [line for line in file if ' _ _ B-PER' in line]\n",
    "b_loc = [line for line in file if ' _ _ B-LOC' in line]\n",
    "i_prod = [line for line in file if ' _ _ I-PROD' in line]\n",
    "i_grp = [line for line in file if ' _ _ I-GRP' in line]\n",
    "i_corp = [line for line in file if ' _ _ I-CORP' in line]\n",
    "i_cw = [line for line in file if ' _ _ I-CW' in line]\n",
    "i_per = [line for line in file if ' _ _ I-PER' in line]\n",
    "i_loc = [line for line in file if ' _ _ I-LOC' in line]\n",
    "\n",
    "#dictionary of labels\n",
    "lwtr = {'O': o_label, \n",
    "        'B-PROD': b_prod,\n",
    "        'B-GRP': b_grp,\n",
    "        'B-CORP': b_corp,\n",
    "        'B-CW': b_cw,\n",
    "        'B-PER': b_per,\n",
    "        'B-LOC': b_loc,\n",
    "        'I-PROD': i_prod,\n",
    "        'I-GRP': i_grp,\n",
    "        'I-CORP': i_corp,\n",
    "        'I-CW': i_cw,\n",
    "        'I-PER': i_per,\n",
    "        'I-LOC': i_loc}\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "for index, line in enumerate(file): #traverse through file. index:value\n",
    "    x = random.binomial(n=1, p=0.5, size=1) #randomizer. x will be 0 or 1\n",
    "    #x==1 means successly random / startswith() is to filter out id lines / line.strip() is to filter out '\\n' lines\n",
    "    if x == 1 and not line.startswith('# id ') and line.strip(): \n",
    "        curr_label = line.split(' _ _ ')[1].strip() #label found in current line. will be used as the key for lwtr dict\n",
    "        lwtr_file[index] = random.choice(lwtr[curr_label]) #access lwtr dict to randomly choose replacement token of same label and reassign it to current line in file\n",
    "\n",
    "lwtr_file\n",
    "\n",
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/TR-Turkish/tr_train_lwtr.conll', 'w') as file:\n",
    "    for line in lwtr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-incidence",
   "metadata": {},
   "source": [
    "# Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "governmental-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/ZH-Chinese/zh_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "#each dataset should have 15,300 training sentences\n",
    "count = 0\n",
    "for i in file:\n",
    "    if i.startswith('# id '):\n",
    "        count += 1\n",
    "assert count == 15300\n",
    "\n",
    "lwtr_file = file.copy() #make new list to prevent overwritting original file-list\n",
    "\n",
    "\n",
    "#for Label-wise Token Replacement, we need to catalogue each possible label\n",
    "o_label = [line for line in file if ' _ _ O' in line]\n",
    "b_prod = [line for line in file if ' _ _ B-PROD' in line]\n",
    "b_grp = [line for line in file if ' _ _ B-GRP' in line]\n",
    "b_corp = [line for line in file if ' _ _ B-CORP' in line]\n",
    "b_cw = [line for line in file if ' _ _ B-CW' in line]\n",
    "b_per = [line for line in file if ' _ _ B-PER' in line]\n",
    "b_loc = [line for line in file if ' _ _ B-LOC' in line]\n",
    "i_prod = [line for line in file if ' _ _ I-PROD' in line]\n",
    "i_grp = [line for line in file if ' _ _ I-GRP' in line]\n",
    "i_corp = [line for line in file if ' _ _ I-CORP' in line]\n",
    "i_cw = [line for line in file if ' _ _ I-CW' in line]\n",
    "i_per = [line for line in file if ' _ _ I-PER' in line]\n",
    "i_loc = [line for line in file if ' _ _ I-LOC' in line]\n",
    "\n",
    "#dictionary of labels\n",
    "lwtr = {'O': o_label, \n",
    "        'B-PROD': b_prod,\n",
    "        'B-GRP': b_grp,\n",
    "        'B-CORP': b_corp,\n",
    "        'B-CW': b_cw,\n",
    "        'B-PER': b_per,\n",
    "        'B-LOC': b_loc,\n",
    "        'I-PROD': i_prod,\n",
    "        'I-GRP': i_grp,\n",
    "        'I-CORP': i_corp,\n",
    "        'I-CW': i_cw,\n",
    "        'I-PER': i_per,\n",
    "        'I-LOC': i_loc}\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "for index, line in enumerate(file): #traverse through file. index:value\n",
    "    x = random.binomial(n=1, p=0.5, size=1) #randomizer. x will be 0 or 1\n",
    "    #x==1 means successly random / startswith() is to filter out id lines / line.strip() is to filter out '\\n' lines\n",
    "    if x == 1 and not line.startswith('# id ') and line.strip(): \n",
    "        curr_label = line.split(' _ _ ')[1].strip() #label found in current line. will be used as the key for lwtr dict\n",
    "        lwtr_file[index] = random.choice(lwtr[curr_label]) #access lwtr dict to randomly choose replacement token of same label and reassign it to current line in file\n",
    "\n",
    "lwtr_file\n",
    "\n",
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/ZH-Chinese/zh_train_lwtr.conll', 'w') as file:\n",
    "    for line in lwtr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-cincinnati",
   "metadata": {},
   "source": [
    "# SR - Synonym Replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-representative",
   "metadata": {},
   "source": [
    "From Adel & Dai:\n",
    "\n",
    "Synonym replacement (SR): Our second approach is similar to LwTR, except that we replace the\n",
    "token with one of its synonyms retrieved from WordNet. Note that the retrieved synonym may consist of\n",
    "more than one token. However, its BIO-labels can be derived using a simple rule: If the replaced token\n",
    "is the first token within a mention (i.e., the corresponding label is ‘B-EntityType’), we assign the same\n",
    "label to the first token of the retrieved multi-word synonym, and ‘I-EntityType’ to the other tokens. If the\n",
    "replaced token is inside a mention (i.e., the corresponding label is ‘I-EntityType’), we assign its label to\n",
    "all tokens of the multi-word synonym.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-scottish",
   "metadata": {},
   "source": [
    "https://fasttext.cc/docs/en/pretrained-vectors.html\n",
    "    \n",
    "https://radimrehurek.com/gensim/models/fasttext.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "significant-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syn_replace(train_file, word_vector):\n",
    "    from numpy import random\n",
    "    from gensim.test.utils import datapath\n",
    "    from gensim.models.fasttext import load_facebook_vectors\n",
    "    \n",
    "    wv = load_facebook_vectors(word_vector) #load language word vector\n",
    "    \n",
    "    sr_file = [] #eventual file to write into .conll \n",
    "    aug_count = 0\n",
    "    no_aug_count = 0\n",
    "    for index, line in enumerate(file):\n",
    "        if not line.startswith('# id ') and line.strip(): #filter out '#id' and '\\n'\n",
    "            x = random.binomial(n=1, p=0.5, size=1) #random coin flip, 1 is augment, 0 is no augmentation\n",
    "            if x == 1: #augment by replacing current word with the most similar (synonym) word in word vector\n",
    "                separate = line.split(' _ _ ') #to get a list of just the word with the label\n",
    "                word = separate[0] #isolate the word in the line, without it's label\n",
    "                synonym = wv.most_similar(word)[0][0] #synonym of isolated word. 'most_similar' returns list of most top 10 most similar words, with corresponding percentage likelihoods. we just need the MOST likely ([0]), without the percentage ([0])\n",
    "                separate[0] = synonym #replace original word with its new synonym\n",
    "                sr_file.append(' _ _ '.join(separate)) #join back together in format of 'synonym _ _ label'\n",
    "                \n",
    "                aug_count += 1\n",
    "                \n",
    "            else: #no augmentation, still keep original line\n",
    "                sr_file.append(line)\n",
    "                \n",
    "                no_aug_count += 1\n",
    "        else: #append #id's and \\n\n",
    "            sr_file.append(line)\n",
    "    print(f'{aug_count} instances of synonym replacement.')\n",
    "    print(f'{no_aug_count} instances of no synonym replacement.')\n",
    "    return sr_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-charleston",
   "metadata": {},
   "source": [
    "# Bangla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "static-cycling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96233 instances of synonym replacement.\n",
      "95664 instances of no synonym replacement.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/BN-Bangla/bn_train.conll') as f:\n",
    "    file = f.readlines() #original bangla training data\n",
    "\n",
    "sr_file = syn_replace(file, 'SemEval2022-Task11_Train-Dev/BN-Bangla/wiki.bn/wiki.bn.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "continent-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/BN-Bangla/bn_train_sr.conll', 'w') as file:\n",
    "    for line in sr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-secretariat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "grateful-florist",
   "metadata": {},
   "source": [
    "# German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "corrected-anxiety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109267 instances of synonym replacement.\n",
      "109455 instances of no synonym replacement.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/DE-German/de_train.conll') as f:\n",
    "    file = f.readlines() #original bangla training data\n",
    "\n",
    "sr_file = syn_replace(file, 'SemEval2022-Task11_Train-Dev/DE-German/wiki.de/wiki.de.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "agreed-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/DE-German/de_train_sr.conll', 'w') as file:\n",
    "    for line in sr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-effort",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "noble-acrobat",
   "metadata": {},
   "source": [
    "# Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "laughing-tennis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111393 instances of synonym replacement.\n",
      "111310 instances of no synonym replacement.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/KO-Korean/ko_train.conll') as f:\n",
    "    file = f.readlines() #original bangla training data\n",
    "\n",
    "sr_file = syn_replace(file, 'SemEval2022-Task11_Train-Dev/KO-Korean/wiki.ko/wiki.ko.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "listed-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "with open('SemEval2022-Task11_Train-Dev/KO-Korean/ko_train_sr.conll', 'w') as file:\n",
    "    for line in sr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-pacific",
   "metadata": {},
   "source": [
    "# MR - Mention Replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-athens",
   "metadata": {},
   "source": [
    "From Adel & Dai:\n",
    "\n",
    "Mention replacement (MR): For each mention in the instance, we use a binomial distribution to\n",
    "randomly decide whether it should be replaced. If yes, we randomly select another mention from the\n",
    "original training set which has the same entity type as the replacement. The corresponding BIO-label\n",
    "sequence can be changed accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-johnston",
   "metadata": {},
   "source": [
    "# Bangla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "dietary-bristol",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############## DATA STRUCTURES SETUP ###############\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/BN-Bangla/bn_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "mention_rep = defaultdict(list) #create default dictionary of lists\n",
    "entities = ['PER', 'LOC', 'GRP', 'CORP', 'PROD', 'CW'] #all basic NER labels\n",
    "\n",
    "begin = False #variable for when you've first encountered and captured the head of a mention (B-)\n",
    "inside = False #variable for when you're inside the mention (I-). There can be 1+ of these\n",
    "\n",
    "for entity in entities: #traverse through the entities\n",
    "    for line in file: #traverse through each line of the language file\n",
    "        if begin: #check to see if you've already run into the head of the mention\n",
    "            if f'I-{entity}' in line: #if not, then see if the I- is in this line\n",
    "                temp_list.append(line) #if so, you must've gone to the 'elif' below and created the temp_list varible. Append current I- onto this list\n",
    "                inside = True #switch to True to alert that you're inside the mention from now on\n",
    "            \n",
    "            else: #if there's no I- in this line that means you've 1) got the head, and 2) either gotten all I-'s already or there's no more I-'s, and you're done trying to capture a mention\n",
    "                if mention_rep[entity]: #check to see if you're already started storing values into this entity (PER, LOC, etc.)\n",
    "                    mention_rep[entity].append(temp_list) #if so, simply append the temp_list onto the existing list\n",
    "                else: #if not that means this is your first entry for an entity's values. defaultdict(list) can't perform append when this happens...\n",
    "                    mention_rep[entity] = [temp_list] #...so we'll have to store the temp_list as a list to prevent the temp_list's contents from being broken into individual values and stored into the dict's value list\n",
    "                begin = False #revert begin to False for next loop\n",
    "                inside = False #revert inside for the same reason\n",
    "            \n",
    "        elif f'B-{entity}' in line: #if you haven't already run into the head of the mention (B-), check to see if it's in this current loop/line\n",
    "            temp_list = [] #create a temporary list that you'll use to collect a mention's beginning (B-) and insides (I-). this will also reset the temp_list for subsequent loops \n",
    "            temp_list.append(line) #add the beginning labels (B-) to this list\n",
    "            begin = True #switch to True to alert that you've found the head label\n",
    "\n",
    "#assert every mention was stored as a list\n",
    "for i in mention_rep: #traverse the dict's keys\n",
    "    for j in mention_rep[i]: #traverse each value in the value-list of the key\n",
    "        assert type(j) is list #check if every value is a list. this is to ensure consistency later on\n",
    "\n",
    "\n",
    "################################################# PERFORM MENTION REPLACEMENT ########################################################\n",
    "import numpy as np #will need ndarray\n",
    "\n",
    "mr_file = []\n",
    "labels = tuple(mention_rep.keys()) #set of labels: PER, LOC, GRP, CORP, PROD, CW\n",
    "\n",
    "#capture_inside = False\n",
    "for index, line in enumerate(file): #traverse through file by line\n",
    "    #print('line:', line)\n",
    "    if line.strip().endswith(labels): #endswith() is to filter out anything other than mentions\n",
    "        if line.strip().split(' _ _ ')[1].startswith('B'): #only get the beginnings of mentions (B-)\n",
    "            #print('is label')\n",
    "            x = random.binomial(n = 1, p = 0.5, size = 1) #randomizer. x will be 0 or 1\n",
    "            #print(f'x = {x}')\n",
    "            if x == 1: #x==1 means successfully random, perform mention replacement\n",
    "                curr_label = line.split(' _ _ ')[1].strip()[2:] #current label type will be used as key next. '2:' is to remove the 'B-' or 'I-' from beginning of label\n",
    "                #print(f'curr_label: {curr_label}')\n",
    "                new_mention = np.random.choice(np.array(mention_rep[curr_label], dtype = 'object')) #have to make this ndarray because the values of our mention_rep[key] dictionary is lists of LISTS. it becomes deprecated. same logic applies as LwTR above\n",
    "                #print(f'new_mention: {new_mention}')\n",
    "                if len(new_mention) == 1: #just insert sole element\n",
    "                    #print('len 1')\n",
    "                    mr_file.append(new_mention[0])\n",
    "                    #print(f'{index+1}: {mr_file}')\n",
    "                else: #multi-word mention\n",
    "                    #print('multi-word')\n",
    "                    for mention in new_mention: #traverse through new_mention list\n",
    "                        #test_file.insert(index+idx, mention) #subsequently insert each part of new_mention \n",
    "                        mr_file.append(mention)\n",
    "                        #print(f'{index+1}: {mr_file}\\n')\n",
    "            else: #B-, but no data augmentation\n",
    "                #print('label, but no data augmentation')\n",
    "                mr_file.append(line) #add line (non-replaced mention beginning (B-))\n",
    "                capture_inside = True #this will enable capturing the inside (I-) of the mention if it appears next, instead of disregarding it\n",
    "                #print(f'{index+1}: {mr_file}\\n')\n",
    "        elif capture_inside: #add the inside of mention, because we just had a non-replaced mention beginning (B-), so we must capture all insides of original mention\n",
    "            #print('unsuccessful mention replacement. capturing current inside mention, I-')\n",
    "            mr_file.append(line) #append non mention replaced inside (I-)\n",
    "            #print(f'{index+1}: {mr_file}\\n')\n",
    "#         else:\n",
    "#             print('sucessful mention replacement. not capturing current inside of mention')\n",
    "            \n",
    "    else: #everything that doesn't have a mention label in it\n",
    "        #print('not label')\n",
    "        mr_file.append(line) #append current line\n",
    "        #print(f'{index+1}: {mr_file}\\n')\n",
    "        capture_inside = False #switch off the flag to indicate we must capture a mention inside\n",
    "\n",
    "        \n",
    "############################################################### CHECKING ############################################################\n",
    "new_count = 0\n",
    "for i in mr_file:\n",
    "    if i.startswith('# id '):\n",
    "        new_count += 1\n",
    "assert original_count == new_count == 15300\n",
    "\n",
    "############################################################ WRITING TO NEW FILE #####################################################\n",
    "with open('SemEval2022-Task11_Train-Dev/BN-Bangla/bn_train_mr.conll', 'w') as file:\n",
    "    for line in mr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-determination",
   "metadata": {},
   "source": [
    "# German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "small-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DATA STRUCTURES SETUP ###############\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/DE-German/de_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "mention_rep = defaultdict(list) #create default dictionary of lists\n",
    "entities = ['PER', 'LOC', 'GRP', 'CORP', 'PROD', 'CW'] #all basic NER labels\n",
    "\n",
    "begin = False #variable for when you've first encountered and captured the head of a mention (B-)\n",
    "inside = False #variable for when you're inside the mention (I-). There can be 1+ of these\n",
    "\n",
    "for entity in entities: #traverse through the entities\n",
    "    for line in file: #traverse through each line of the language file\n",
    "        if begin: #check to see if you've already run into the head of the mention\n",
    "            if f'I-{entity}' in line: #if not, then see if the I- is in this line\n",
    "                temp_list.append(line) #if so, you must've gone to the 'elif' below and created the temp_list varible. Append current I- onto this list\n",
    "                inside = True #switch to True to alert that you're inside the mention from now on\n",
    "            \n",
    "            else: #if there's no I- in this line that means you've 1) got the head, and 2) either gotten all I-'s already or there's no more I-'s, and you're done trying to capture a mention\n",
    "                if mention_rep[entity]: #check to see if you're already started storing values into this entity (PER, LOC, etc.)\n",
    "                    mention_rep[entity].append(temp_list) #if so, simply append the temp_list onto the existing list\n",
    "                else: #if not that means this is your first entry for an entity's values. defaultdict(list) can't perform append when this happens...\n",
    "                    mention_rep[entity] = [temp_list] #...so we'll have to store the temp_list as a list to prevent the temp_list's contents from being broken into individual values and stored into the dict's value list\n",
    "                begin = False #revert begin to False for next loop\n",
    "                inside = False #revert inside for the same reason\n",
    "            \n",
    "        elif f'B-{entity}' in line: #if you haven't already run into the head of the mention (B-), check to see if it's in this current loop/line\n",
    "            temp_list = [] #create a temporary list that you'll use to collect a mention's beginning (B-) and insides (I-). this will also reset the temp_list for subsequent loops \n",
    "            temp_list.append(line) #add the beginning labels (B-) to this list\n",
    "            begin = True #switch to True to alert that you've found the head label\n",
    "\n",
    "#assert every mention was stored as a list\n",
    "for i in mention_rep: #traverse the dict's keys\n",
    "    for j in mention_rep[i]: #traverse each value in the value-list of the key\n",
    "        assert type(j) is list #check if every value is a list. this is to ensure consistency later on\n",
    "\n",
    "\n",
    "################################################# PERFORM MENTION REPLACEMENT ########################################################\n",
    "import numpy as np #will need ndarray\n",
    "\n",
    "mr_file = []\n",
    "labels = tuple(mention_rep.keys()) #set of labels: PER, LOC, GRP, CORP, PROD, CW\n",
    "\n",
    "#capture_inside = False\n",
    "for index, line in enumerate(file): #traverse through file by line\n",
    "    #print('line:', line)\n",
    "    if line.strip().endswith(labels): #endswith() is to filter out anything other than mentions\n",
    "        if line.strip().split(' _ _ ')[1].startswith('B'): #only get the beginnings of mentions (B-)\n",
    "            #print('is label')\n",
    "            x = random.binomial(n = 1, p = 0.5, size = 1) #randomizer. x will be 0 or 1\n",
    "            #print(f'x = {x}')\n",
    "            if x == 1: #x==1 means successfully random, perform mention replacement\n",
    "                curr_label = line.split(' _ _ ')[1].strip()[2:] #current label type will be used as key next. '2:' is to remove the 'B-' or 'I-' from beginning of label\n",
    "                #print(f'curr_label: {curr_label}')\n",
    "                new_mention = np.random.choice(np.array(mention_rep[curr_label], dtype = 'object')) #have to make this ndarray because the values of our mention_rep[key] dictionary is lists of LISTS. it becomes deprecated. same logic applies as LwTR above\n",
    "                #print(f'new_mention: {new_mention}')\n",
    "                if len(new_mention) == 1: #just insert sole element\n",
    "                    #print('len 1')\n",
    "                    mr_file.append(new_mention[0])\n",
    "                    #print(f'{index+1}: {mr_file}')\n",
    "                else: #multi-word mention\n",
    "                    #print('multi-word')\n",
    "                    for mention in new_mention: #traverse through new_mention list\n",
    "                        #test_file.insert(index+idx, mention) #subsequently insert each part of new_mention \n",
    "                        mr_file.append(mention)\n",
    "                        #print(f'{index+1}: {mr_file}\\n')\n",
    "            else: #B-, but no data augmentation\n",
    "                #print('label, but no data augmentation')\n",
    "                mr_file.append(line) #add line (non-replaced mention beginning (B-))\n",
    "                capture_inside = True #this will enable capturing the inside (I-) of the mention if it appears next, instead of disregarding it\n",
    "                #print(f'{index+1}: {mr_file}\\n')\n",
    "        elif capture_inside: #add the inside of mention, because we just had a non-replaced mention beginning (B-), so we must capture all insides of original mention\n",
    "            #print('unsuccessful mention replacement. capturing current inside mention, I-')\n",
    "            mr_file.append(line) #append non mention replaced inside (I-)\n",
    "            #print(f'{index+1}: {mr_file}\\n')\n",
    "#         else:\n",
    "#             print('sucessful mention replacement. not capturing current inside of mention')\n",
    "            \n",
    "    else: #everything that doesn't have a mention label in it\n",
    "        #print('not label')\n",
    "        mr_file.append(line) #append current line\n",
    "        #print(f'{index+1}: {mr_file}\\n')\n",
    "        capture_inside = False #switch off the flag to indicate we must capture a mention inside\n",
    "\n",
    "        \n",
    "############################################################### CHECKING ############################################################\n",
    "new_count = 0\n",
    "for i in mr_file:\n",
    "    if i.startswith('# id '):\n",
    "        new_count += 1\n",
    "assert original_count == new_count == 15300\n",
    "\n",
    "############################################################ WRITING TO NEW FILE #####################################################\n",
    "with open('SemEval2022-Task11_Train-Dev/DE-German/de_train_mr.conll', 'w') as file:\n",
    "    for line in mr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-browser",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "occasional-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DATA STRUCTURES SETUP ###############\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/EN-English/en_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "mention_rep = defaultdict(list) #create default dictionary of lists\n",
    "entities = ['PER', 'LOC', 'GRP', 'CORP', 'PROD', 'CW'] #all basic NER labels\n",
    "\n",
    "begin = False #variable for when you've first encountered and captured the head of a mention (B-)\n",
    "inside = False #variable for when you're inside the mention (I-). There can be 1+ of these\n",
    "\n",
    "for entity in entities: #traverse through the entities\n",
    "    for line in file: #traverse through each line of the language file\n",
    "        if begin: #check to see if you've already run into the head of the mention\n",
    "            if f'I-{entity}' in line: #if not, then see if the I- is in this line\n",
    "                temp_list.append(line) #if so, you must've gone to the 'elif' below and created the temp_list varible. Append current I- onto this list\n",
    "                inside = True #switch to True to alert that you're inside the mention from now on\n",
    "            \n",
    "            else: #if there's no I- in this line that means you've 1) got the head, and 2) either gotten all I-'s already or there's no more I-'s, and you're done trying to capture a mention\n",
    "                if mention_rep[entity]: #check to see if you're already started storing values into this entity (PER, LOC, etc.)\n",
    "                    mention_rep[entity].append(temp_list) #if so, simply append the temp_list onto the existing list\n",
    "                else: #if not that means this is your first entry for an entity's values. defaultdict(list) can't perform append when this happens...\n",
    "                    mention_rep[entity] = [temp_list] #...so we'll have to store the temp_list as a list to prevent the temp_list's contents from being broken into individual values and stored into the dict's value list\n",
    "                begin = False #revert begin to False for next loop\n",
    "                inside = False #revert inside for the same reason\n",
    "            \n",
    "        elif f'B-{entity}' in line: #if you haven't already run into the head of the mention (B-), check to see if it's in this current loop/line\n",
    "            temp_list = [] #create a temporary list that you'll use to collect a mention's beginning (B-) and insides (I-). this will also reset the temp_list for subsequent loops \n",
    "            temp_list.append(line) #add the beginning labels (B-) to this list\n",
    "            begin = True #switch to True to alert that you've found the head label\n",
    "\n",
    "#assert every mention was stored as a list\n",
    "for i in mention_rep: #traverse the dict's keys\n",
    "    for j in mention_rep[i]: #traverse each value in the value-list of the key\n",
    "        assert type(j) is list #check if every value is a list. this is to ensure consistency later on\n",
    "\n",
    "\n",
    "################################################# PERFORM MENTION REPLACEMENT ########################################################\n",
    "import numpy as np #will need ndarray\n",
    "\n",
    "mr_file = []\n",
    "labels = tuple(mention_rep.keys()) #set of labels: PER, LOC, GRP, CORP, PROD, CW\n",
    "\n",
    "#capture_inside = False\n",
    "for index, line in enumerate(file): #traverse through file by line\n",
    "    #print('line:', line)\n",
    "    if line.strip().endswith(labels): #endswith() is to filter out anything other than mentions\n",
    "        if line.strip().split(' _ _ ')[1].startswith('B'): #only get the beginnings of mentions (B-)\n",
    "            #print('is label')\n",
    "            x = random.binomial(n = 1, p = 0.5, size = 1) #randomizer. x will be 0 or 1\n",
    "            #print(f'x = {x}')\n",
    "            if x == 1: #x==1 means successfully random, perform mention replacement\n",
    "                curr_label = line.split(' _ _ ')[1].strip()[2:] #current label type will be used as key next. '2:' is to remove the 'B-' or 'I-' from beginning of label\n",
    "                #print(f'curr_label: {curr_label}')\n",
    "                new_mention = np.random.choice(np.array(mention_rep[curr_label], dtype = 'object')) #have to make this ndarray because the values of our mention_rep[key] dictionary is lists of LISTS. it becomes deprecated. same logic applies as LwTR above\n",
    "                #print(f'new_mention: {new_mention}')\n",
    "                if len(new_mention) == 1: #just insert sole element\n",
    "                    #print('len 1')\n",
    "                    mr_file.append(new_mention[0])\n",
    "                    #print(f'{index+1}: {mr_file}')\n",
    "                else: #multi-word mention\n",
    "                    #print('multi-word')\n",
    "                    for mention in new_mention: #traverse through new_mention list\n",
    "                        #test_file.insert(index+idx, mention) #subsequently insert each part of new_mention \n",
    "                        mr_file.append(mention)\n",
    "                        #print(f'{index+1}: {mr_file}\\n')\n",
    "            else: #B-, but no data augmentation\n",
    "                #print('label, but no data augmentation')\n",
    "                mr_file.append(line) #add line (non-replaced mention beginning (B-))\n",
    "                capture_inside = True #this will enable capturing the inside (I-) of the mention if it appears next, instead of disregarding it\n",
    "                #print(f'{index+1}: {mr_file}\\n')\n",
    "        elif capture_inside: #add the inside of mention, because we just had a non-replaced mention beginning (B-), so we must capture all insides of original mention\n",
    "            #print('unsuccessful mention replacement. capturing current inside mention, I-')\n",
    "            mr_file.append(line) #append non mention replaced inside (I-)\n",
    "            #print(f'{index+1}: {mr_file}\\n')\n",
    "#         else:\n",
    "#             print('sucessful mention replacement. not capturing current inside of mention')\n",
    "            \n",
    "    else: #everything that doesn't have a mention label in it\n",
    "        #print('not label')\n",
    "        mr_file.append(line) #append current line\n",
    "        #print(f'{index+1}: {mr_file}\\n')\n",
    "        capture_inside = False #switch off the flag to indicate we must capture a mention inside\n",
    "\n",
    "        \n",
    "############################################################### CHECKING ############################################################\n",
    "new_count = 0\n",
    "for i in mr_file:\n",
    "    if i.startswith('# id '):\n",
    "        new_count += 1\n",
    "assert original_count == new_count == 15300\n",
    "\n",
    "############################################################ WRITING TO NEW FILE #####################################################\n",
    "with open('SemEval2022-Task11_Train-Dev/EN-English/en_train_mr.conll', 'w') as file:\n",
    "    for line in mr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-search",
   "metadata": {},
   "source": [
    "# Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "whole-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DATA STRUCTURES SETUP ###############\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/DE-German/de_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "mention_rep = defaultdict(list) #create default dictionary of lists\n",
    "entities = ['PER', 'LOC', 'GRP', 'CORP', 'PROD', 'CW'] #all basic NER labels\n",
    "\n",
    "begin = False #variable for when you've first encountered and captured the head of a mention (B-)\n",
    "inside = False #variable for when you're inside the mention (I-). There can be 1+ of these\n",
    "\n",
    "for entity in entities: #traverse through the entities\n",
    "    for line in file: #traverse through each line of the language file\n",
    "        if begin: #check to see if you've already run into the head of the mention\n",
    "            if f'I-{entity}' in line: #if not, then see if the I- is in this line\n",
    "                temp_list.append(line) #if so, you must've gone to the 'elif' below and created the temp_list varible. Append current I- onto this list\n",
    "                inside = True #switch to True to alert that you're inside the mention from now on\n",
    "            \n",
    "            else: #if there's no I- in this line that means you've 1) got the head, and 2) either gotten all I-'s already or there's no more I-'s, and you're done trying to capture a mention\n",
    "                if mention_rep[entity]: #check to see if you're already started storing values into this entity (PER, LOC, etc.)\n",
    "                    mention_rep[entity].append(temp_list) #if so, simply append the temp_list onto the existing list\n",
    "                else: #if not that means this is your first entry for an entity's values. defaultdict(list) can't perform append when this happens...\n",
    "                    mention_rep[entity] = [temp_list] #...so we'll have to store the temp_list as a list to prevent the temp_list's contents from being broken into individual values and stored into the dict's value list\n",
    "                begin = False #revert begin to False for next loop\n",
    "                inside = False #revert inside for the same reason\n",
    "            \n",
    "        elif f'B-{entity}' in line: #if you haven't already run into the head of the mention (B-), check to see if it's in this current loop/line\n",
    "            temp_list = [] #create a temporary list that you'll use to collect a mention's beginning (B-) and insides (I-). this will also reset the temp_list for subsequent loops \n",
    "            temp_list.append(line) #add the beginning labels (B-) to this list\n",
    "            begin = True #switch to True to alert that you've found the head label\n",
    "\n",
    "#assert every mention was stored as a list\n",
    "for i in mention_rep: #traverse the dict's keys\n",
    "    for j in mention_rep[i]: #traverse each value in the value-list of the key\n",
    "        assert type(j) is list #check if every value is a list. this is to ensure consistency later on\n",
    "\n",
    "\n",
    "################################################# PERFORM MENTION REPLACEMENT ########################################################\n",
    "import numpy as np #will need ndarray\n",
    "\n",
    "mr_file = []\n",
    "labels = tuple(mention_rep.keys()) #set of labels: PER, LOC, GRP, CORP, PROD, CW\n",
    "\n",
    "#capture_inside = False\n",
    "for index, line in enumerate(file): #traverse through file by line\n",
    "    #print('line:', line)\n",
    "    if line.strip().endswith(labels): #endswith() is to filter out anything other than mentions\n",
    "        if line.strip().split(' _ _ ')[1].startswith('B'): #only get the beginnings of mentions (B-)\n",
    "            #print('is label')\n",
    "            x = random.binomial(n = 1, p = 0.5, size = 1) #randomizer. x will be 0 or 1\n",
    "            #print(f'x = {x}')\n",
    "            if x == 1: #x==1 means successfully random, perform mention replacement\n",
    "                curr_label = line.split(' _ _ ')[1].strip()[2:] #current label type will be used as key next. '2:' is to remove the 'B-' or 'I-' from beginning of label\n",
    "                #print(f'curr_label: {curr_label}')\n",
    "                new_mention = np.random.choice(np.array(mention_rep[curr_label], dtype = 'object')) #have to make this ndarray because the values of our mention_rep[key] dictionary is lists of LISTS. it becomes deprecated. same logic applies as LwTR above\n",
    "                #print(f'new_mention: {new_mention}')\n",
    "                if len(new_mention) == 1: #just insert sole element\n",
    "                    #print('len 1')\n",
    "                    mr_file.append(new_mention[0])\n",
    "                    #print(f'{index+1}: {mr_file}')\n",
    "                else: #multi-word mention\n",
    "                    #print('multi-word')\n",
    "                    for mention in new_mention: #traverse through new_mention list\n",
    "                        #test_file.insert(index+idx, mention) #subsequently insert each part of new_mention \n",
    "                        mr_file.append(mention)\n",
    "                        #print(f'{index+1}: {mr_file}\\n')\n",
    "            else: #B-, but no data augmentation\n",
    "                #print('label, but no data augmentation')\n",
    "                mr_file.append(line) #add line (non-replaced mention beginning (B-))\n",
    "                capture_inside = True #this will enable capturing the inside (I-) of the mention if it appears next, instead of disregarding it\n",
    "                #print(f'{index+1}: {mr_file}\\n')\n",
    "        elif capture_inside: #add the inside of mention, because we just had a non-replaced mention beginning (B-), so we must capture all insides of original mention\n",
    "            #print('unsuccessful mention replacement. capturing current inside mention, I-')\n",
    "            mr_file.append(line) #append non mention replaced inside (I-)\n",
    "            #print(f'{index+1}: {mr_file}\\n')\n",
    "#         else:\n",
    "#             print('sucessful mention replacement. not capturing current inside of mention')\n",
    "            \n",
    "    else: #everything that doesn't have a mention label in it\n",
    "        #print('not label')\n",
    "        mr_file.append(line) #append current line\n",
    "        #print(f'{index+1}: {mr_file}\\n')\n",
    "        capture_inside = False #switch off the flag to indicate we must capture a mention inside\n",
    "\n",
    "        \n",
    "############################################################### CHECKING ############################################################\n",
    "new_count = 0\n",
    "for i in mr_file:\n",
    "    if i.startswith('# id '):\n",
    "        new_count += 1\n",
    "assert original_count == new_count == 15300\n",
    "\n",
    "############################################################ WRITING TO NEW FILE #####################################################\n",
    "with open('SemEval2022-Task11_Train-Dev/ES-Spanish/es_train_mr.conll', 'w') as file:\n",
    "    for line in mr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-guard",
   "metadata": {},
   "source": [
    "# Farsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "persistent-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DATA STRUCTURES SETUP ###############\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/FA-Farsi/fa_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "mention_rep = defaultdict(list) #create default dictionary of lists\n",
    "entities = ['PER', 'LOC', 'GRP', 'CORP', 'PROD', 'CW'] #all basic NER labels\n",
    "\n",
    "begin = False #variable for when you've first encountered and captured the head of a mention (B-)\n",
    "inside = False #variable for when you're inside the mention (I-). There can be 1+ of these\n",
    "\n",
    "for entity in entities: #traverse through the entities\n",
    "    for line in file: #traverse through each line of the language file\n",
    "        if begin: #check to see if you've already run into the head of the mention\n",
    "            if f'I-{entity}' in line: #if not, then see if the I- is in this line\n",
    "                temp_list.append(line) #if so, you must've gone to the 'elif' below and created the temp_list varible. Append current I- onto this list\n",
    "                inside = True #switch to True to alert that you're inside the mention from now on\n",
    "            \n",
    "            else: #if there's no I- in this line that means you've 1) got the head, and 2) either gotten all I-'s already or there's no more I-'s, and you're done trying to capture a mention\n",
    "                if mention_rep[entity]: #check to see if you're already started storing values into this entity (PER, LOC, etc.)\n",
    "                    mention_rep[entity].append(temp_list) #if so, simply append the temp_list onto the existing list\n",
    "                else: #if not that means this is your first entry for an entity's values. defaultdict(list) can't perform append when this happens...\n",
    "                    mention_rep[entity] = [temp_list] #...so we'll have to store the temp_list as a list to prevent the temp_list's contents from being broken into individual values and stored into the dict's value list\n",
    "                begin = False #revert begin to False for next loop\n",
    "                inside = False #revert inside for the same reason\n",
    "            \n",
    "        elif f'B-{entity}' in line: #if you haven't already run into the head of the mention (B-), check to see if it's in this current loop/line\n",
    "            temp_list = [] #create a temporary list that you'll use to collect a mention's beginning (B-) and insides (I-). this will also reset the temp_list for subsequent loops \n",
    "            temp_list.append(line) #add the beginning labels (B-) to this list\n",
    "            begin = True #switch to True to alert that you've found the head label\n",
    "\n",
    "#assert every mention was stored as a list\n",
    "for i in mention_rep: #traverse the dict's keys\n",
    "    for j in mention_rep[i]: #traverse each value in the value-list of the key\n",
    "        assert type(j) is list #check if every value is a list. this is to ensure consistency later on\n",
    "\n",
    "\n",
    "################################################# PERFORM MENTION REPLACEMENT ########################################################\n",
    "import numpy as np #will need ndarray\n",
    "\n",
    "mr_file = []\n",
    "labels = tuple(mention_rep.keys()) #set of labels: PER, LOC, GRP, CORP, PROD, CW\n",
    "\n",
    "#capture_inside = False\n",
    "for index, line in enumerate(file): #traverse through file by line\n",
    "    #print('line:', line)\n",
    "    if line.strip().endswith(labels): #endswith() is to filter out anything other than mentions\n",
    "        if line.strip().split(' _ _ ')[1].startswith('B'): #only get the beginnings of mentions (B-)\n",
    "            #print('is label')\n",
    "            x = random.binomial(n = 1, p = 0.5, size = 1) #randomizer. x will be 0 or 1\n",
    "            #print(f'x = {x}')\n",
    "            if x == 1: #x==1 means successfully random, perform mention replacement\n",
    "                curr_label = line.split(' _ _ ')[1].strip()[2:] #current label type will be used as key next. '2:' is to remove the 'B-' or 'I-' from beginning of label\n",
    "                #print(f'curr_label: {curr_label}')\n",
    "                new_mention = np.random.choice(np.array(mention_rep[curr_label], dtype = 'object')) #have to make this ndarray because the values of our mention_rep[key] dictionary is lists of LISTS. it becomes deprecated. same logic applies as LwTR above\n",
    "                #print(f'new_mention: {new_mention}')\n",
    "                if len(new_mention) == 1: #just insert sole element\n",
    "                    #print('len 1')\n",
    "                    mr_file.append(new_mention[0])\n",
    "                    #print(f'{index+1}: {mr_file}')\n",
    "                else: #multi-word mention\n",
    "                    #print('multi-word')\n",
    "                    for mention in new_mention: #traverse through new_mention list\n",
    "                        #test_file.insert(index+idx, mention) #subsequently insert each part of new_mention \n",
    "                        mr_file.append(mention)\n",
    "                        #print(f'{index+1}: {mr_file}\\n')\n",
    "            else: #B-, but no data augmentation\n",
    "                #print('label, but no data augmentation')\n",
    "                mr_file.append(line) #add line (non-replaced mention beginning (B-))\n",
    "                capture_inside = True #this will enable capturing the inside (I-) of the mention if it appears next, instead of disregarding it\n",
    "                #print(f'{index+1}: {mr_file}\\n')\n",
    "        elif capture_inside: #add the inside of mention, because we just had a non-replaced mention beginning (B-), so we must capture all insides of original mention\n",
    "            #print('unsuccessful mention replacement. capturing current inside mention, I-')\n",
    "            mr_file.append(line) #append non mention replaced inside (I-)\n",
    "            #print(f'{index+1}: {mr_file}\\n')\n",
    "#         else:\n",
    "#             print('sucessful mention replacement. not capturing current inside of mention')\n",
    "            \n",
    "    else: #everything that doesn't have a mention label in it\n",
    "        #print('not label')\n",
    "        mr_file.append(line) #append current line\n",
    "        #print(f'{index+1}: {mr_file}\\n')\n",
    "        capture_inside = False #switch off the flag to indicate we must capture a mention inside\n",
    "\n",
    "        \n",
    "############################################################### CHECKING ############################################################\n",
    "new_count = 0\n",
    "for i in mr_file:\n",
    "    if i.startswith('# id '):\n",
    "        new_count += 1\n",
    "assert original_count == new_count == 15300\n",
    "\n",
    "############################################################ WRITING TO NEW FILE #####################################################\n",
    "with open('SemEval2022-Task11_Train-Dev/FA-Farsi/fa_train_mr.conll', 'w') as file:\n",
    "    for line in mr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-album",
   "metadata": {},
   "source": [
    "# Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "qualified-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DATA STRUCTURES SETUP ###############\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/HI-Hindi/hi_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "mention_rep = defaultdict(list) #create default dictionary of lists\n",
    "entities = ['PER', 'LOC', 'GRP', 'CORP', 'PROD', 'CW'] #all basic NER labels\n",
    "\n",
    "begin = False #variable for when you've first encountered and captured the head of a mention (B-)\n",
    "inside = False #variable for when you're inside the mention (I-). There can be 1+ of these\n",
    "\n",
    "for entity in entities: #traverse through the entities\n",
    "    for line in file: #traverse through each line of the language file\n",
    "        if begin: #check to see if you've already run into the head of the mention\n",
    "            if f'I-{entity}' in line: #if not, then see if the I- is in this line\n",
    "                temp_list.append(line) #if so, you must've gone to the 'elif' below and created the temp_list varible. Append current I- onto this list\n",
    "                inside = True #switch to True to alert that you're inside the mention from now on\n",
    "            \n",
    "            else: #if there's no I- in this line that means you've 1) got the head, and 2) either gotten all I-'s already or there's no more I-'s, and you're done trying to capture a mention\n",
    "                if mention_rep[entity]: #check to see if you're already started storing values into this entity (PER, LOC, etc.)\n",
    "                    mention_rep[entity].append(temp_list) #if so, simply append the temp_list onto the existing list\n",
    "                else: #if not that means this is your first entry for an entity's values. defaultdict(list) can't perform append when this happens...\n",
    "                    mention_rep[entity] = [temp_list] #...so we'll have to store the temp_list as a list to prevent the temp_list's contents from being broken into individual values and stored into the dict's value list\n",
    "                begin = False #revert begin to False for next loop\n",
    "                inside = False #revert inside for the same reason\n",
    "            \n",
    "        elif f'B-{entity}' in line: #if you haven't already run into the head of the mention (B-), check to see if it's in this current loop/line\n",
    "            temp_list = [] #create a temporary list that you'll use to collect a mention's beginning (B-) and insides (I-). this will also reset the temp_list for subsequent loops \n",
    "            temp_list.append(line) #add the beginning labels (B-) to this list\n",
    "            begin = True #switch to True to alert that you've found the head label\n",
    "\n",
    "#assert every mention was stored as a list\n",
    "for i in mention_rep: #traverse the dict's keys\n",
    "    for j in mention_rep[i]: #traverse each value in the value-list of the key\n",
    "        assert type(j) is list #check if every value is a list. this is to ensure consistency later on\n",
    "\n",
    "\n",
    "################################################# PERFORM MENTION REPLACEMENT ########################################################\n",
    "import numpy as np #will need ndarray\n",
    "\n",
    "mr_file = []\n",
    "labels = tuple(mention_rep.keys()) #set of labels: PER, LOC, GRP, CORP, PROD, CW\n",
    "\n",
    "#capture_inside = False\n",
    "for index, line in enumerate(file): #traverse through file by line\n",
    "    #print('line:', line)\n",
    "    if line.strip().endswith(labels): #endswith() is to filter out anything other than mentions\n",
    "        if line.strip().split(' _ _ ')[1].startswith('B'): #only get the beginnings of mentions (B-)\n",
    "            #print('is label')\n",
    "            x = random.binomial(n = 1, p = 0.5, size = 1) #randomizer. x will be 0 or 1\n",
    "            #print(f'x = {x}')\n",
    "            if x == 1: #x==1 means successfully random, perform mention replacement\n",
    "                curr_label = line.split(' _ _ ')[1].strip()[2:] #current label type will be used as key next. '2:' is to remove the 'B-' or 'I-' from beginning of label\n",
    "                #print(f'curr_label: {curr_label}')\n",
    "                new_mention = np.random.choice(np.array(mention_rep[curr_label], dtype = 'object')) #have to make this ndarray because the values of our mention_rep[key] dictionary is lists of LISTS. it becomes deprecated. same logic applies as LwTR above\n",
    "                #print(f'new_mention: {new_mention}')\n",
    "                if len(new_mention) == 1: #just insert sole element\n",
    "                    #print('len 1')\n",
    "                    mr_file.append(new_mention[0])\n",
    "                    #print(f'{index+1}: {mr_file}')\n",
    "                else: #multi-word mention\n",
    "                    #print('multi-word')\n",
    "                    for mention in new_mention: #traverse through new_mention list\n",
    "                        #test_file.insert(index+idx, mention) #subsequently insert each part of new_mention \n",
    "                        mr_file.append(mention)\n",
    "                        #print(f'{index+1}: {mr_file}\\n')\n",
    "            else: #B-, but no data augmentation\n",
    "                #print('label, but no data augmentation')\n",
    "                mr_file.append(line) #add line (non-replaced mention beginning (B-))\n",
    "                capture_inside = True #this will enable capturing the inside (I-) of the mention if it appears next, instead of disregarding it\n",
    "                #print(f'{index+1}: {mr_file}\\n')\n",
    "        elif capture_inside: #add the inside of mention, because we just had a non-replaced mention beginning (B-), so we must capture all insides of original mention\n",
    "            #print('unsuccessful mention replacement. capturing current inside mention, I-')\n",
    "            mr_file.append(line) #append non mention replaced inside (I-)\n",
    "            #print(f'{index+1}: {mr_file}\\n')\n",
    "#         else:\n",
    "#             print('sucessful mention replacement. not capturing current inside of mention')\n",
    "            \n",
    "    else: #everything that doesn't have a mention label in it\n",
    "        #print('not label')\n",
    "        mr_file.append(line) #append current line\n",
    "        #print(f'{index+1}: {mr_file}\\n')\n",
    "        capture_inside = False #switch off the flag to indicate we must capture a mention inside\n",
    "\n",
    "        \n",
    "############################################################### CHECKING ############################################################\n",
    "new_count = 0\n",
    "for i in mr_file:\n",
    "    if i.startswith('# id '):\n",
    "        new_count += 1\n",
    "assert original_count == new_count == 15300\n",
    "\n",
    "############################################################ WRITING TO NEW FILE #####################################################\n",
    "with open('SemEval2022-Task11_Train-Dev/HI-Hindi/hi_train_mr.conll', 'w') as file:\n",
    "    for line in mr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-collection",
   "metadata": {},
   "source": [
    "# Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "becoming-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DATA STRUCTURES SETUP ###############\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/KO-Korean/ko_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "mention_rep = defaultdict(list) #create default dictionary of lists\n",
    "entities = ['PER', 'LOC', 'GRP', 'CORP', 'PROD', 'CW'] #all basic NER labels\n",
    "\n",
    "begin = False #variable for when you've first encountered and captured the head of a mention (B-)\n",
    "inside = False #variable for when you're inside the mention (I-). There can be 1+ of these\n",
    "\n",
    "for entity in entities: #traverse through the entities\n",
    "    for line in file: #traverse through each line of the language file\n",
    "        if begin: #check to see if you've already run into the head of the mention\n",
    "            if f'I-{entity}' in line: #if not, then see if the I- is in this line\n",
    "                temp_list.append(line) #if so, you must've gone to the 'elif' below and created the temp_list varible. Append current I- onto this list\n",
    "                inside = True #switch to True to alert that you're inside the mention from now on\n",
    "            \n",
    "            else: #if there's no I- in this line that means you've 1) got the head, and 2) either gotten all I-'s already or there's no more I-'s, and you're done trying to capture a mention\n",
    "                if mention_rep[entity]: #check to see if you're already started storing values into this entity (PER, LOC, etc.)\n",
    "                    mention_rep[entity].append(temp_list) #if so, simply append the temp_list onto the existing list\n",
    "                else: #if not that means this is your first entry for an entity's values. defaultdict(list) can't perform append when this happens...\n",
    "                    mention_rep[entity] = [temp_list] #...so we'll have to store the temp_list as a list to prevent the temp_list's contents from being broken into individual values and stored into the dict's value list\n",
    "                begin = False #revert begin to False for next loop\n",
    "                inside = False #revert inside for the same reason\n",
    "            \n",
    "        elif f'B-{entity}' in line: #if you haven't already run into the head of the mention (B-), check to see if it's in this current loop/line\n",
    "            temp_list = [] #create a temporary list that you'll use to collect a mention's beginning (B-) and insides (I-). this will also reset the temp_list for subsequent loops \n",
    "            temp_list.append(line) #add the beginning labels (B-) to this list\n",
    "            begin = True #switch to True to alert that you've found the head label\n",
    "\n",
    "#assert every mention was stored as a list\n",
    "for i in mention_rep: #traverse the dict's keys\n",
    "    for j in mention_rep[i]: #traverse each value in the value-list of the key\n",
    "        assert type(j) is list #check if every value is a list. this is to ensure consistency later on\n",
    "\n",
    "\n",
    "################################################# PERFORM MENTION REPLACEMENT ########################################################\n",
    "import numpy as np #will need ndarray\n",
    "\n",
    "mr_file = []\n",
    "labels = tuple(mention_rep.keys()) #set of labels: PER, LOC, GRP, CORP, PROD, CW\n",
    "\n",
    "#capture_inside = False\n",
    "for index, line in enumerate(file): #traverse through file by line\n",
    "    #print('line:', line)\n",
    "    if line.strip().endswith(labels): #endswith() is to filter out anything other than mentions\n",
    "        if line.strip().split(' _ _ ')[1].startswith('B'): #only get the beginnings of mentions (B-)\n",
    "            #print('is label')\n",
    "            x = random.binomial(n = 1, p = 0.5, size = 1) #randomizer. x will be 0 or 1\n",
    "            #print(f'x = {x}')\n",
    "            if x == 1: #x==1 means successfully random, perform mention replacement\n",
    "                curr_label = line.split(' _ _ ')[1].strip()[2:] #current label type will be used as key next. '2:' is to remove the 'B-' or 'I-' from beginning of label\n",
    "                #print(f'curr_label: {curr_label}')\n",
    "                new_mention = np.random.choice(np.array(mention_rep[curr_label], dtype = 'object')) #have to make this ndarray because the values of our mention_rep[key] dictionary is lists of LISTS. it becomes deprecated. same logic applies as LwTR above\n",
    "                #print(f'new_mention: {new_mention}')\n",
    "                if len(new_mention) == 1: #just insert sole element\n",
    "                    #print('len 1')\n",
    "                    mr_file.append(new_mention[0])\n",
    "                    #print(f'{index+1}: {mr_file}')\n",
    "                else: #multi-word mention\n",
    "                    #print('multi-word')\n",
    "                    for mention in new_mention: #traverse through new_mention list\n",
    "                        #test_file.insert(index+idx, mention) #subsequently insert each part of new_mention \n",
    "                        mr_file.append(mention)\n",
    "                        #print(f'{index+1}: {mr_file}\\n')\n",
    "            else: #B-, but no data augmentation\n",
    "                #print('label, but no data augmentation')\n",
    "                mr_file.append(line) #add line (non-replaced mention beginning (B-))\n",
    "                capture_inside = True #this will enable capturing the inside (I-) of the mention if it appears next, instead of disregarding it\n",
    "                #print(f'{index+1}: {mr_file}\\n')\n",
    "        elif capture_inside: #add the inside of mention, because we just had a non-replaced mention beginning (B-), so we must capture all insides of original mention\n",
    "            #print('unsuccessful mention replacement. capturing current inside mention, I-')\n",
    "            mr_file.append(line) #append non mention replaced inside (I-)\n",
    "            #print(f'{index+1}: {mr_file}\\n')\n",
    "#         else:\n",
    "#             print('sucessful mention replacement. not capturing current inside of mention')\n",
    "            \n",
    "    else: #everything that doesn't have a mention label in it\n",
    "        #print('not label')\n",
    "        mr_file.append(line) #append current line\n",
    "        #print(f'{index+1}: {mr_file}\\n')\n",
    "        capture_inside = False #switch off the flag to indicate we must capture a mention inside\n",
    "\n",
    "        \n",
    "############################################################### CHECKING ############################################################\n",
    "new_count = 0\n",
    "for i in mr_file:\n",
    "    if i.startswith('# id '):\n",
    "        new_count += 1\n",
    "assert original_count == new_count == 15300\n",
    "\n",
    "############################################################ WRITING TO NEW FILE #####################################################\n",
    "with open('SemEval2022-Task11_Train-Dev/KO-Korean/ko_train_mr.conll', 'w') as file:\n",
    "    for line in mr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-calibration",
   "metadata": {},
   "source": [
    "# Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "behind-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DATA STRUCTURES SETUP ###############\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/NL-Dutch/nl_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "mention_rep = defaultdict(list) #create default dictionary of lists\n",
    "entities = ['PER', 'LOC', 'GRP', 'CORP', 'PROD', 'CW'] #all basic NER labels\n",
    "\n",
    "begin = False #variable for when you've first encountered and captured the head of a mention (B-)\n",
    "inside = False #variable for when you're inside the mention (I-). There can be 1+ of these\n",
    "\n",
    "for entity in entities: #traverse through the entities\n",
    "    for line in file: #traverse through each line of the language file\n",
    "        if begin: #check to see if you've already run into the head of the mention\n",
    "            if f'I-{entity}' in line: #if not, then see if the I- is in this line\n",
    "                temp_list.append(line) #if so, you must've gone to the 'elif' below and created the temp_list varible. Append current I- onto this list\n",
    "                inside = True #switch to True to alert that you're inside the mention from now on\n",
    "            \n",
    "            else: #if there's no I- in this line that means you've 1) got the head, and 2) either gotten all I-'s already or there's no more I-'s, and you're done trying to capture a mention\n",
    "                if mention_rep[entity]: #check to see if you're already started storing values into this entity (PER, LOC, etc.)\n",
    "                    mention_rep[entity].append(temp_list) #if so, simply append the temp_list onto the existing list\n",
    "                else: #if not that means this is your first entry for an entity's values. defaultdict(list) can't perform append when this happens...\n",
    "                    mention_rep[entity] = [temp_list] #...so we'll have to store the temp_list as a list to prevent the temp_list's contents from being broken into individual values and stored into the dict's value list\n",
    "                begin = False #revert begin to False for next loop\n",
    "                inside = False #revert inside for the same reason\n",
    "            \n",
    "        elif f'B-{entity}' in line: #if you haven't already run into the head of the mention (B-), check to see if it's in this current loop/line\n",
    "            temp_list = [] #create a temporary list that you'll use to collect a mention's beginning (B-) and insides (I-). this will also reset the temp_list for subsequent loops \n",
    "            temp_list.append(line) #add the beginning labels (B-) to this list\n",
    "            begin = True #switch to True to alert that you've found the head label\n",
    "\n",
    "#assert every mention was stored as a list\n",
    "for i in mention_rep: #traverse the dict's keys\n",
    "    for j in mention_rep[i]: #traverse each value in the value-list of the key\n",
    "        assert type(j) is list #check if every value is a list. this is to ensure consistency later on\n",
    "\n",
    "\n",
    "################################################# PERFORM MENTION REPLACEMENT ########################################################\n",
    "import numpy as np #will need ndarray\n",
    "\n",
    "mr_file = []\n",
    "labels = tuple(mention_rep.keys()) #set of labels: PER, LOC, GRP, CORP, PROD, CW\n",
    "\n",
    "#capture_inside = False\n",
    "for index, line in enumerate(file): #traverse through file by line\n",
    "    #print('line:', line)\n",
    "    if line.strip().endswith(labels): #endswith() is to filter out anything other than mentions\n",
    "        if line.strip().split(' _ _ ')[1].startswith('B'): #only get the beginnings of mentions (B-)\n",
    "            #print('is label')\n",
    "            x = random.binomial(n = 1, p = 0.5, size = 1) #randomizer. x will be 0 or 1\n",
    "            #print(f'x = {x}')\n",
    "            if x == 1: #x==1 means successfully random, perform mention replacement\n",
    "                curr_label = line.split(' _ _ ')[1].strip()[2:] #current label type will be used as key next. '2:' is to remove the 'B-' or 'I-' from beginning of label\n",
    "                #print(f'curr_label: {curr_label}')\n",
    "                new_mention = np.random.choice(np.array(mention_rep[curr_label], dtype = 'object')) #have to make this ndarray because the values of our mention_rep[key] dictionary is lists of LISTS. it becomes deprecated. same logic applies as LwTR above\n",
    "                #print(f'new_mention: {new_mention}')\n",
    "                if len(new_mention) == 1: #just insert sole element\n",
    "                    #print('len 1')\n",
    "                    mr_file.append(new_mention[0])\n",
    "                    #print(f'{index+1}: {mr_file}')\n",
    "                else: #multi-word mention\n",
    "                    #print('multi-word')\n",
    "                    for mention in new_mention: #traverse through new_mention list\n",
    "                        #test_file.insert(index+idx, mention) #subsequently insert each part of new_mention \n",
    "                        mr_file.append(mention)\n",
    "                        #print(f'{index+1}: {mr_file}\\n')\n",
    "            else: #B-, but no data augmentation\n",
    "                #print('label, but no data augmentation')\n",
    "                mr_file.append(line) #add line (non-replaced mention beginning (B-))\n",
    "                capture_inside = True #this will enable capturing the inside (I-) of the mention if it appears next, instead of disregarding it\n",
    "                #print(f'{index+1}: {mr_file}\\n')\n",
    "        elif capture_inside: #add the inside of mention, because we just had a non-replaced mention beginning (B-), so we must capture all insides of original mention\n",
    "            #print('unsuccessful mention replacement. capturing current inside mention, I-')\n",
    "            mr_file.append(line) #append non mention replaced inside (I-)\n",
    "            #print(f'{index+1}: {mr_file}\\n')\n",
    "#         else:\n",
    "#             print('sucessful mention replacement. not capturing current inside of mention')\n",
    "            \n",
    "    else: #everything that doesn't have a mention label in it\n",
    "        #print('not label')\n",
    "        mr_file.append(line) #append current line\n",
    "        #print(f'{index+1}: {mr_file}\\n')\n",
    "        capture_inside = False #switch off the flag to indicate we must capture a mention inside\n",
    "\n",
    "        \n",
    "############################################################### CHECKING ############################################################\n",
    "new_count = 0\n",
    "for i in mr_file:\n",
    "    if i.startswith('# id '):\n",
    "        new_count += 1\n",
    "assert original_count == new_count == 15300\n",
    "\n",
    "############################################################ WRITING TO NEW FILE #####################################################\n",
    "with open('SemEval2022-Task11_Train-Dev/NL-Dutch/nl_train_mr.conll', 'w') as file:\n",
    "    for line in mr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-mauritius",
   "metadata": {},
   "source": [
    "# Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "sustained-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DATA STRUCTURES SETUP ###############\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/RU-Russian/ru_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "mention_rep = defaultdict(list) #create default dictionary of lists\n",
    "entities = ['PER', 'LOC', 'GRP', 'CORP', 'PROD', 'CW'] #all basic NER labels\n",
    "\n",
    "begin = False #variable for when you've first encountered and captured the head of a mention (B-)\n",
    "inside = False #variable for when you're inside the mention (I-). There can be 1+ of these\n",
    "\n",
    "for entity in entities: #traverse through the entities\n",
    "    for line in file: #traverse through each line of the language file\n",
    "        if begin: #check to see if you've already run into the head of the mention\n",
    "            if f'I-{entity}' in line: #if not, then see if the I- is in this line\n",
    "                temp_list.append(line) #if so, you must've gone to the 'elif' below and created the temp_list varible. Append current I- onto this list\n",
    "                inside = True #switch to True to alert that you're inside the mention from now on\n",
    "            \n",
    "            else: #if there's no I- in this line that means you've 1) got the head, and 2) either gotten all I-'s already or there's no more I-'s, and you're done trying to capture a mention\n",
    "                if mention_rep[entity]: #check to see if you're already started storing values into this entity (PER, LOC, etc.)\n",
    "                    mention_rep[entity].append(temp_list) #if so, simply append the temp_list onto the existing list\n",
    "                else: #if not that means this is your first entry for an entity's values. defaultdict(list) can't perform append when this happens...\n",
    "                    mention_rep[entity] = [temp_list] #...so we'll have to store the temp_list as a list to prevent the temp_list's contents from being broken into individual values and stored into the dict's value list\n",
    "                begin = False #revert begin to False for next loop\n",
    "                inside = False #revert inside for the same reason\n",
    "            \n",
    "        elif f'B-{entity}' in line: #if you haven't already run into the head of the mention (B-), check to see if it's in this current loop/line\n",
    "            temp_list = [] #create a temporary list that you'll use to collect a mention's beginning (B-) and insides (I-). this will also reset the temp_list for subsequent loops \n",
    "            temp_list.append(line) #add the beginning labels (B-) to this list\n",
    "            begin = True #switch to True to alert that you've found the head label\n",
    "\n",
    "#assert every mention was stored as a list\n",
    "for i in mention_rep: #traverse the dict's keys\n",
    "    for j in mention_rep[i]: #traverse each value in the value-list of the key\n",
    "        assert type(j) is list #check if every value is a list. this is to ensure consistency later on\n",
    "\n",
    "\n",
    "################################################# PERFORM MENTION REPLACEMENT ########################################################\n",
    "import numpy as np #will need ndarray\n",
    "\n",
    "mr_file = []\n",
    "labels = tuple(mention_rep.keys()) #set of labels: PER, LOC, GRP, CORP, PROD, CW\n",
    "\n",
    "#capture_inside = False\n",
    "for index, line in enumerate(file): #traverse through file by line\n",
    "    #print('line:', line)\n",
    "    if line.strip().endswith(labels): #endswith() is to filter out anything other than mentions\n",
    "        if line.strip().split(' _ _ ')[1].startswith('B'): #only get the beginnings of mentions (B-)\n",
    "            #print('is label')\n",
    "            x = random.binomial(n = 1, p = 0.5, size = 1) #randomizer. x will be 0 or 1\n",
    "            #print(f'x = {x}')\n",
    "            if x == 1: #x==1 means successfully random, perform mention replacement\n",
    "                curr_label = line.split(' _ _ ')[1].strip()[2:] #current label type will be used as key next. '2:' is to remove the 'B-' or 'I-' from beginning of label\n",
    "                #print(f'curr_label: {curr_label}')\n",
    "                new_mention = np.random.choice(np.array(mention_rep[curr_label], dtype = 'object')) #have to make this ndarray because the values of our mention_rep[key] dictionary is lists of LISTS. it becomes deprecated. same logic applies as LwTR above\n",
    "                #print(f'new_mention: {new_mention}')\n",
    "                if len(new_mention) == 1: #just insert sole element\n",
    "                    #print('len 1')\n",
    "                    mr_file.append(new_mention[0])\n",
    "                    #print(f'{index+1}: {mr_file}')\n",
    "                else: #multi-word mention\n",
    "                    #print('multi-word')\n",
    "                    for mention in new_mention: #traverse through new_mention list\n",
    "                        #test_file.insert(index+idx, mention) #subsequently insert each part of new_mention \n",
    "                        mr_file.append(mention)\n",
    "                        #print(f'{index+1}: {mr_file}\\n')\n",
    "            else: #B-, but no data augmentation\n",
    "                #print('label, but no data augmentation')\n",
    "                mr_file.append(line) #add line (non-replaced mention beginning (B-))\n",
    "                capture_inside = True #this will enable capturing the inside (I-) of the mention if it appears next, instead of disregarding it\n",
    "                #print(f'{index+1}: {mr_file}\\n')\n",
    "        elif capture_inside: #add the inside of mention, because we just had a non-replaced mention beginning (B-), so we must capture all insides of original mention\n",
    "            #print('unsuccessful mention replacement. capturing current inside mention, I-')\n",
    "            mr_file.append(line) #append non mention replaced inside (I-)\n",
    "            #print(f'{index+1}: {mr_file}\\n')\n",
    "#         else:\n",
    "#             print('sucessful mention replacement. not capturing current inside of mention')\n",
    "            \n",
    "    else: #everything that doesn't have a mention label in it\n",
    "        #print('not label')\n",
    "        mr_file.append(line) #append current line\n",
    "        #print(f'{index+1}: {mr_file}\\n')\n",
    "        capture_inside = False #switch off the flag to indicate we must capture a mention inside\n",
    "\n",
    "        \n",
    "############################################################### CHECKING ############################################################\n",
    "new_count = 0\n",
    "for i in mr_file:\n",
    "    if i.startswith('# id '):\n",
    "        new_count += 1\n",
    "assert original_count == new_count == 15300\n",
    "\n",
    "############################################################ WRITING TO NEW FILE #####################################################\n",
    "with open('SemEval2022-Task11_Train-Dev/RU-Russian/ru_train_mr.conll', 'w') as file:\n",
    "    for line in mr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-grammar",
   "metadata": {},
   "source": [
    "# Turkish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "attended-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DATA STRUCTURES SETUP ###############\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/TR-Turkish/tr_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "mention_rep = defaultdict(list) #create default dictionary of lists\n",
    "entities = ['PER', 'LOC', 'GRP', 'CORP', 'PROD', 'CW'] #all basic NER labels\n",
    "\n",
    "begin = False #variable for when you've first encountered and captured the head of a mention (B-)\n",
    "inside = False #variable for when you're inside the mention (I-). There can be 1+ of these\n",
    "\n",
    "for entity in entities: #traverse through the entities\n",
    "    for line in file: #traverse through each line of the language file\n",
    "        if begin: #check to see if you've already run into the head of the mention\n",
    "            if f'I-{entity}' in line: #if not, then see if the I- is in this line\n",
    "                temp_list.append(line) #if so, you must've gone to the 'elif' below and created the temp_list varible. Append current I- onto this list\n",
    "                inside = True #switch to True to alert that you're inside the mention from now on\n",
    "            \n",
    "            else: #if there's no I- in this line that means you've 1) got the head, and 2) either gotten all I-'s already or there's no more I-'s, and you're done trying to capture a mention\n",
    "                if mention_rep[entity]: #check to see if you're already started storing values into this entity (PER, LOC, etc.)\n",
    "                    mention_rep[entity].append(temp_list) #if so, simply append the temp_list onto the existing list\n",
    "                else: #if not that means this is your first entry for an entity's values. defaultdict(list) can't perform append when this happens...\n",
    "                    mention_rep[entity] = [temp_list] #...so we'll have to store the temp_list as a list to prevent the temp_list's contents from being broken into individual values and stored into the dict's value list\n",
    "                begin = False #revert begin to False for next loop\n",
    "                inside = False #revert inside for the same reason\n",
    "            \n",
    "        elif f'B-{entity}' in line: #if you haven't already run into the head of the mention (B-), check to see if it's in this current loop/line\n",
    "            temp_list = [] #create a temporary list that you'll use to collect a mention's beginning (B-) and insides (I-). this will also reset the temp_list for subsequent loops \n",
    "            temp_list.append(line) #add the beginning labels (B-) to this list\n",
    "            begin = True #switch to True to alert that you've found the head label\n",
    "\n",
    "#assert every mention was stored as a list\n",
    "for i in mention_rep: #traverse the dict's keys\n",
    "    for j in mention_rep[i]: #traverse each value in the value-list of the key\n",
    "        assert type(j) is list #check if every value is a list. this is to ensure consistency later on\n",
    "\n",
    "\n",
    "################################################# PERFORM MENTION REPLACEMENT ########################################################\n",
    "import numpy as np #will need ndarray\n",
    "\n",
    "mr_file = []\n",
    "labels = tuple(mention_rep.keys()) #set of labels: PER, LOC, GRP, CORP, PROD, CW\n",
    "\n",
    "#capture_inside = False\n",
    "for index, line in enumerate(file): #traverse through file by line\n",
    "    #print('line:', line)\n",
    "    if line.strip().endswith(labels): #endswith() is to filter out anything other than mentions\n",
    "        if line.strip().split(' _ _ ')[1].startswith('B'): #only get the beginnings of mentions (B-)\n",
    "            #print('is label')\n",
    "            x = random.binomial(n = 1, p = 0.5, size = 1) #randomizer. x will be 0 or 1\n",
    "            #print(f'x = {x}')\n",
    "            if x == 1: #x==1 means successfully random, perform mention replacement\n",
    "                curr_label = line.split(' _ _ ')[1].strip()[2:] #current label type will be used as key next. '2:' is to remove the 'B-' or 'I-' from beginning of label\n",
    "                #print(f'curr_label: {curr_label}')\n",
    "                new_mention = np.random.choice(np.array(mention_rep[curr_label], dtype = 'object')) #have to make this ndarray because the values of our mention_rep[key] dictionary is lists of LISTS. it becomes deprecated. same logic applies as LwTR above\n",
    "                #print(f'new_mention: {new_mention}')\n",
    "                if len(new_mention) == 1: #just insert sole element\n",
    "                    #print('len 1')\n",
    "                    mr_file.append(new_mention[0])\n",
    "                    #print(f'{index+1}: {mr_file}')\n",
    "                else: #multi-word mention\n",
    "                    #print('multi-word')\n",
    "                    for mention in new_mention: #traverse through new_mention list\n",
    "                        #test_file.insert(index+idx, mention) #subsequently insert each part of new_mention \n",
    "                        mr_file.append(mention)\n",
    "                        #print(f'{index+1}: {mr_file}\\n')\n",
    "            else: #B-, but no data augmentation\n",
    "                #print('label, but no data augmentation')\n",
    "                mr_file.append(line) #add line (non-replaced mention beginning (B-))\n",
    "                capture_inside = True #this will enable capturing the inside (I-) of the mention if it appears next, instead of disregarding it\n",
    "                #print(f'{index+1}: {mr_file}\\n')\n",
    "        elif capture_inside: #add the inside of mention, because we just had a non-replaced mention beginning (B-), so we must capture all insides of original mention\n",
    "            #print('unsuccessful mention replacement. capturing current inside mention, I-')\n",
    "            mr_file.append(line) #append non mention replaced inside (I-)\n",
    "            #print(f'{index+1}: {mr_file}\\n')\n",
    "#         else:\n",
    "#             print('sucessful mention replacement. not capturing current inside of mention')\n",
    "            \n",
    "    else: #everything that doesn't have a mention label in it\n",
    "        #print('not label')\n",
    "        mr_file.append(line) #append current line\n",
    "        #print(f'{index+1}: {mr_file}\\n')\n",
    "        capture_inside = False #switch off the flag to indicate we must capture a mention inside\n",
    "\n",
    "        \n",
    "############################################################### CHECKING ############################################################\n",
    "new_count = 0\n",
    "for i in mr_file:\n",
    "    if i.startswith('# id '):\n",
    "        new_count += 1\n",
    "assert original_count == new_count == 15300\n",
    "\n",
    "############################################################ WRITING TO NEW FILE #####################################################\n",
    "with open('SemEval2022-Task11_Train-Dev/TR-Turkish/tr_train_mr.conll', 'w') as file:\n",
    "    for line in mr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-median",
   "metadata": {},
   "source": [
    "# Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "backed-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DATA STRUCTURES SETUP ###############\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/ZH-Chinese/zh_train.conll') as f:\n",
    "    file = [i for i in f.readlines()]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "mention_rep = defaultdict(list) #create default dictionary of lists\n",
    "entities = ['PER', 'LOC', 'GRP', 'CORP', 'PROD', 'CW'] #all basic NER labels\n",
    "\n",
    "begin = False #variable for when you've first encountered and captured the head of a mention (B-)\n",
    "inside = False #variable for when you're inside the mention (I-). There can be 1+ of these\n",
    "\n",
    "for entity in entities: #traverse through the entities\n",
    "    for line in file: #traverse through each line of the language file\n",
    "        if begin: #check to see if you've already run into the head of the mention\n",
    "            if f'I-{entity}' in line: #if not, then see if the I- is in this line\n",
    "                temp_list.append(line) #if so, you must've gone to the 'elif' below and created the temp_list varible. Append current I- onto this list\n",
    "                inside = True #switch to True to alert that you're inside the mention from now on\n",
    "            \n",
    "            else: #if there's no I- in this line that means you've 1) got the head, and 2) either gotten all I-'s already or there's no more I-'s, and you're done trying to capture a mention\n",
    "                if mention_rep[entity]: #check to see if you're already started storing values into this entity (PER, LOC, etc.)\n",
    "                    mention_rep[entity].append(temp_list) #if so, simply append the temp_list onto the existing list\n",
    "                else: #if not that means this is your first entry for an entity's values. defaultdict(list) can't perform append when this happens...\n",
    "                    mention_rep[entity] = [temp_list] #...so we'll have to store the temp_list as a list to prevent the temp_list's contents from being broken into individual values and stored into the dict's value list\n",
    "                begin = False #revert begin to False for next loop\n",
    "                inside = False #revert inside for the same reason\n",
    "            \n",
    "        elif f'B-{entity}' in line: #if you haven't already run into the head of the mention (B-), check to see if it's in this current loop/line\n",
    "            temp_list = [] #create a temporary list that you'll use to collect a mention's beginning (B-) and insides (I-). this will also reset the temp_list for subsequent loops \n",
    "            temp_list.append(line) #add the beginning labels (B-) to this list\n",
    "            begin = True #switch to True to alert that you've found the head label\n",
    "\n",
    "#assert every mention was stored as a list\n",
    "for i in mention_rep: #traverse the dict's keys\n",
    "    for j in mention_rep[i]: #traverse each value in the value-list of the key\n",
    "        assert type(j) is list #check if every value is a list. this is to ensure consistency later on\n",
    "\n",
    "\n",
    "################################################# PERFORM MENTION REPLACEMENT ########################################################\n",
    "import numpy as np #will need ndarray\n",
    "\n",
    "mr_file = []\n",
    "labels = tuple(mention_rep.keys()) #set of labels: PER, LOC, GRP, CORP, PROD, CW\n",
    "\n",
    "#capture_inside = False\n",
    "for index, line in enumerate(file): #traverse through file by line\n",
    "    #print('line:', line)\n",
    "    if line.strip().endswith(labels): #endswith() is to filter out anything other than mentions\n",
    "        if line.strip().split(' _ _ ')[1].startswith('B'): #only get the beginnings of mentions (B-)\n",
    "            #print('is label')\n",
    "            x = random.binomial(n = 1, p = 0.5, size = 1) #randomizer. x will be 0 or 1\n",
    "            #print(f'x = {x}')\n",
    "            if x == 1: #x==1 means successfully random, perform mention replacement\n",
    "                curr_label = line.split(' _ _ ')[1].strip()[2:] #current label type will be used as key next. '2:' is to remove the 'B-' or 'I-' from beginning of label\n",
    "                #print(f'curr_label: {curr_label}')\n",
    "                new_mention = np.random.choice(np.array(mention_rep[curr_label], dtype = 'object')) #have to make this ndarray because the values of our mention_rep[key] dictionary is lists of LISTS. it becomes deprecated. same logic applies as LwTR above\n",
    "                #print(f'new_mention: {new_mention}')\n",
    "                if len(new_mention) == 1: #just insert sole element\n",
    "                    #print('len 1')\n",
    "                    mr_file.append(new_mention[0])\n",
    "                    #print(f'{index+1}: {mr_file}')\n",
    "                else: #multi-word mention\n",
    "                    #print('multi-word')\n",
    "                    for mention in new_mention: #traverse through new_mention list\n",
    "                        #test_file.insert(index+idx, mention) #subsequently insert each part of new_mention \n",
    "                        mr_file.append(mention)\n",
    "                        #print(f'{index+1}: {mr_file}\\n')\n",
    "            else: #B-, but no data augmentation\n",
    "                #print('label, but no data augmentation')\n",
    "                mr_file.append(line) #add line (non-replaced mention beginning (B-))\n",
    "                capture_inside = True #this will enable capturing the inside (I-) of the mention if it appears next, instead of disregarding it\n",
    "                #print(f'{index+1}: {mr_file}\\n')\n",
    "        elif capture_inside: #add the inside of mention, because we just had a non-replaced mention beginning (B-), so we must capture all insides of original mention\n",
    "            #print('unsuccessful mention replacement. capturing current inside mention, I-')\n",
    "            mr_file.append(line) #append non mention replaced inside (I-)\n",
    "            #print(f'{index+1}: {mr_file}\\n')\n",
    "#         else:\n",
    "#             print('sucessful mention replacement. not capturing current inside of mention')\n",
    "            \n",
    "    else: #everything that doesn't have a mention label in it\n",
    "        #print('not label')\n",
    "        mr_file.append(line) #append current line\n",
    "        #print(f'{index+1}: {mr_file}\\n')\n",
    "        capture_inside = False #switch off the flag to indicate we must capture a mention inside\n",
    "\n",
    "        \n",
    "############################################################### CHECKING ############################################################\n",
    "new_count = 0\n",
    "for i in mr_file:\n",
    "    if i.startswith('# id '):\n",
    "        new_count += 1\n",
    "assert original_count == new_count == 15300\n",
    "\n",
    "############################################################ WRITING TO NEW FILE #####################################################\n",
    "with open('SemEval2022-Task11_Train-Dev/ZH-Chinese/zh_train_mr.conll', 'w') as file:\n",
    "    for line in mr_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-defense",
   "metadata": {},
   "source": [
    "# SiS - Shuffle within Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-trout",
   "metadata": {},
   "source": [
    "From Adel & Dai:\n",
    "    \n",
    "Shuffle within segments (SiS): We first split the token sequence into segments of the same label. Thus,\n",
    "each segment corresponds to either a mention or a sequence of out-of-mention tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "addressed-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are as follows\n",
    "# PER : Person\n",
    "# LOC : Location\n",
    "# GRP : Group\n",
    "# CORP : Corporation\n",
    "# PROD : Product\n",
    "# CW: Creative Work\n",
    "# O: out-of-mention label (i.e. unrelated to named entity)\n",
    "\n",
    "# B - beginning of label, I - inside label\n",
    "from numpy import random\n",
    "\n",
    "def shuffle(file):\n",
    "    labels = tuple(['-PER\\n', '-LOC\\n', '-GRP\\n', '-CORP\\n', '-PROD\\n', '-CW\\n', ' _ _ O']) #set of labels: PER, LOC, GRP, CORP, PROD, CW, and O\n",
    "    shuffle_file = []\n",
    "    prev_label = None\n",
    "    temp_list = []\n",
    "    shuffle_count = 0\n",
    "    non_shuffle_count = 0\n",
    "    \n",
    "    for index, line in enumerate(file):\n",
    "        #print(f'Loop: {index+1} current line: {line} temp_list: {temp_list}')\n",
    "        #print(f'shuffle_file: {shuffle_file}')\n",
    "        if not line.startswith('# id ') and line.strip(): #to filter out #id's and \\n\n",
    "            for label in labels:\n",
    "                if label in line:\n",
    "                    curr_label = label\n",
    "                    break\n",
    "                    \n",
    "            if curr_label != prev_label and temp_list: #when you've come to a new label, and not on your first loop\n",
    "                x = random.binomial(n=1, p=0.5, size=1)\n",
    "                if x == 1: #perform shuffle data augmnetation\n",
    "                    random.shuffle(temp_list)\n",
    "                    #print('data not augmenting')\n",
    "                    #print(f'temp_list before looping through: {temp_list}')\n",
    "                    for shuff_line in temp_list: #traverse through temp list\n",
    "                        shuffle_file.append(shuff_line)\n",
    "                    temp_list.clear() #reset temp_list\n",
    "                    temp_list.append(line) #begin new temp_list with current new label\n",
    "                    prev_label = curr_label #reassign current label value to prev_label to be used as reference for next loop\n",
    "                    shuffle_count += 1\n",
    "                    #print('data augmenting')\n",
    "                    #print(f'after new temp_list: {temp_list}')\n",
    "                else: #don't shuffle, just append\n",
    "                    #print('data not augmenting')\n",
    "                    #print(f'temp_list before looping through: {temp_list}')\n",
    "                    for original_line in temp_list:\n",
    "                        shuffle_file.append(original_line)\n",
    "                    temp_list.clear() #reset temp_list\n",
    "                    temp_list.append(line) #begin new temp_list with current new label\n",
    "                    prev_label = curr_label #reassign current label value to prev_label to be used as reference for next loop\n",
    "                    non_shuffle_count += 1\n",
    "                    #print('data not augmenting')\n",
    "                    #print(f'after new temp_list: {temp_list}')\n",
    "                #temp_list.append(line)\n",
    "            else: #not on transitionary label. this could mean you're on your first label or another of the same label you've been on\n",
    "                temp_list.append(line) #add current line to temp_list\n",
    "                #print(f'{index+1} {line} prev_label: {prev_label}')\n",
    "                prev_label = curr_label #reassign current label value to prev_label to be used as reference for next loop\n",
    "                #print(f'{index+1} {line} curr_label: {curr_label}')\n",
    "        elif line == '\\n': #end of current #id NER, shuffle and dump current temp_list holdings\n",
    "            x = random.binomial(n=1, p=0.5, size=1)\n",
    "            if x == 1: #perform shuffle data augmnetation\n",
    "                random.shuffle(temp_list)\n",
    "                #print('data not augmenting')\n",
    "                #print(f'temp_list before looping through: {temp_list}')\n",
    "                for shuff_line in temp_list: #traverse through temp list\n",
    "                    shuffle_file.append(shuff_line)\n",
    "                shuffle_file.append(line)\n",
    "                temp_list.clear() #reset temp_list\n",
    "                prev_label = None #reassign current label value to prev_label to be used as reference for next loop\n",
    "                shuffle_count += 1\n",
    "                #print('data augmenting')\n",
    "                #print(f'after new temp_list: {temp_list}')\n",
    "            else: #don't shuffle, just append\n",
    "                #print('data not augmenting')\n",
    "                #print(f'temp_list before looping through: {temp_list}')\n",
    "                for original_line in temp_list:\n",
    "                    shuffle_file.append(original_line)\n",
    "                shuffle_file.append(line)\n",
    "                temp_list.clear() #reset temp_list\n",
    "                prev_label = None #reassig\n",
    "                non_shuffle_count += 1\n",
    "        else:\n",
    "            shuffle_file.append(line) #append #id's and \\n\n",
    "            prev_label = None\n",
    "    print(f'{shuffle_count} instances of shuffling.')\n",
    "    print(f'{non_shuffle_count} instances of not shuffling.')\n",
    "    return shuffle_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-mystery",
   "metadata": {},
   "source": [
    "# Representative Languages: BN, KO, DE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-silly",
   "metadata": {},
   "source": [
    "# Bangla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cutting-booth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21890 instances of shuffling.\n",
      "21737 instances of not shuffling.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/BN-Bangla/bn_train.conll') as f:\n",
    "    file = f.readlines()\n",
    "bn_shuffle_file = shuffle(file)\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/BN-Bangla/bn_train_sis.conll', 'w') as file:\n",
    "    for line in bn_shuffle_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-fairy",
   "metadata": {},
   "source": [
    "# Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "native-professional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27721 instances of shuffling.\n",
      "27416 instances of not shuffling.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/KO-Korean/ko_train.conll') as f:\n",
    "    file = f.readlines()\n",
    "ko_shuffle_file = shuffle(file)\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/KO-Korean/ko_train_sis.conll', 'w') as file:\n",
    "    for line in ko_shuffle_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-result",
   "metadata": {},
   "source": [
    "# German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "touched-broad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28571 instances of shuffling.\n",
      "28287 instances of not shuffling.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/DE-German/de_train.conll') as f:\n",
    "    file = f.readlines()\n",
    "de_shuffle_file = shuffle(file)\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/DE-German/de_train_sis.conll', 'w') as file:\n",
    "    for line in de_shuffle_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-accordance",
   "metadata": {},
   "source": [
    "# Other Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-competition",
   "metadata": {},
   "source": [
    "Turkish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "advised-respondent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28752 instances of shuffling.\n",
      "29116 instances of not shuffling.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/TR-Turkish/tr_train.conll') as f:\n",
    "    file = f.readlines()\n",
    "tr_shuffle_file = shuffle(file)\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/TR-Turkish/tr_train_sis.conll', 'w') as file:\n",
    "    for line in tr_shuffle_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-heading",
   "metadata": {},
   "source": [
    "Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "western-prison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29806 instances of shuffling.\n",
      "29967 instances of not shuffling.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/ZH-Chinese/zh_train.conll') as f:\n",
    "    file = f.readlines()\n",
    "zh_shuffle_file = shuffle(file)\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/ZH-Chinese/zh_train_sis.conll', 'w') as file:\n",
    "    for line in zh_shuffle_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-protest",
   "metadata": {},
   "source": [
    "Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "retired-creativity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22100 instances of shuffling.\n",
      "21787 instances of not shuffling.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/HI-Hindi/hi_train.conll') as f:\n",
    "    file = f.readlines()\n",
    "hi_shuffle_file = shuffle(file)\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/HI-Hindi/hi_train_sis.conll', 'w') as file:\n",
    "    for line in hi_shuffle_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-olive",
   "metadata": {},
   "source": [
    "Farsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "early-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27936 instances of shuffling.\n",
      "27531 instances of not shuffling.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/FA-Farsi/fa_train.conll') as f:\n",
    "    file = f.readlines()\n",
    "fa_shuffle_file = shuffle(file)\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/FA-Farsi/fa_train_sis.conll', 'w') as file:\n",
    "    for line in fa_shuffle_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-administrator",
   "metadata": {},
   "source": [
    "Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "authorized-premiere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26226 instances of shuffling.\n",
      "25948 instances of not shuffling.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/RU-Russian/ru_train.conll') as f:\n",
    "    file = f.readlines()\n",
    "ru_shuffle_file = shuffle(file)\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/RU-Russian/ru_train_sis.conll', 'w') as file:\n",
    "    for line in ru_shuffle_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-williams",
   "metadata": {},
   "source": [
    "Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "complex-preference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28834 instances of shuffling.\n",
      "29065 instances of not shuffling.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/NL-Dutch/nl_train.conll') as f:\n",
    "    file = f.readlines()\n",
    "nl_shuffle_file = shuffle(file)\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/NL-Dutch/nl_train_sis.conll', 'w') as file:\n",
    "    for line in nl_shuffle_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-bibliography",
   "metadata": {},
   "source": [
    "Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "chicken-network",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28962 instances of shuffling.\n",
      "29003 instances of not shuffling.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/ES-Spanish/es_train.conll') as f:\n",
    "    file = f.readlines()\n",
    "es_shuffle_file = shuffle(file)\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/ES-Spanish/es_train_sis.conll', 'w') as file:\n",
    "    for line in es_shuffle_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-alloy",
   "metadata": {},
   "source": [
    "English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "powerful-project",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29220 instances of shuffling.\n",
      "29870 instances of not shuffling.\n"
     ]
    }
   ],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/EN-English/en_train.conll') as f:\n",
    "    file = f.readlines()\n",
    "en_shuffle_file = shuffle(file)\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/EN-English/en_train_sis.conll', 'w') as file:\n",
    "    for line in en_shuffle_file:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/EN-English/en_train.conll') as f:\n",
    "    file = f.readlines()\n",
    "test_shuffle = shuffle(file)\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/EN-English/test_sis.conll', 'w') as file:\n",
    "    for line in test_shuffle:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-ozone",
   "metadata": {},
   "source": [
    "# All"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-atlas",
   "metadata": {},
   "source": [
    "All: We also explore to augment the training set using all aforementioned augmentation methods. That\n",
    "is, for each training instance, we create multiple augmented instances, one per augmentation method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "interior-fashion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR tr\n",
      "ZH zh\n",
      "HI hi\n",
      "FA fa\n",
      "DE de\n",
      "RU ru\n",
      "NL nl\n",
      "ES es\n",
      "EN en\n"
     ]
    }
   ],
   "source": [
    "a = {'TR':('Turkish', 'tr'),\n",
    "    'ZH':('Chinese', 'zh'),\n",
    "    'HI':('Hindi', 'hi'),\n",
    "    'FA':('Farsi', 'fa'),\n",
    "    'DE':('German', 'de'),\n",
    "    'RU':('Russian', 'ru'),\n",
    "    'NL':('Dutch', 'nl'),\n",
    "    'ES':('Spanish', 'es'),\n",
    "    'EN':('English', 'en'),}\n",
    "\n",
    "for i, j in a.items():\n",
    "    print(i, j[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-status",
   "metadata": {},
   "source": [
    "# Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "logical-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'TR':('Turkish', 'tr'),\n",
    "    'ZH':('Chinese', 'zh'),\n",
    "    'HI':('Hindi', 'hi'),\n",
    "    'FA':('Farsi', 'fa'),\n",
    "    'DE':('German', 'de'),\n",
    "    'RU':('Russian', 'ru'),\n",
    "    'NL':('Dutch', 'nl'),\n",
    "    'ES':('Spanish', 'es'),\n",
    "    'EN':('English', 'en')}\n",
    "\n",
    "for i, j in a.items():\n",
    "    with open(f'SemEval2022-Task11_Train-Dev/{i}-{j[0]}/{j[1]}_train.conll') as f:\n",
    "        og_file = f.readlines()\n",
    "    with open(f'SemEval2022-Task11_Train-Dev/{i}-{j[0]}/{j[1]}_train_lwtr.conll') as f:\n",
    "        lwtr_file = f.readlines()\n",
    "    with open(f'SemEval2022-Task11_Train-Dev/{i}-{j[0]}/{j[1]}_train_mr.conll') as f:\n",
    "        mr_file = f.readlines()\n",
    "    with open(f'SemEval2022-Task11_Train-Dev/{i}-{j[0]}/{j[1]}_train_sr.conll') as f:\n",
    "        sr_file = f.readlines()\n",
    "    with open(f'SemEval2022-Task11_Train-Dev/{i}-{j[0]}/{j[1]}_train_sis.conll') as f:\n",
    "        sis_file = f.readlines()\n",
    "\n",
    "    combined_file = og_file + lwtr_file + mr_file + sr_file + sis_file\n",
    "\n",
    "    with open('SemEval2022-Task11_Train-Dev/KO-Korean/ko_train_combined.conll', 'w') as f:\n",
    "        for line in combined_file:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "indonesian-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'SemEval2022-Task11_Train-Dev/ES-Spanish/es_train.conll') as f:\n",
    "    og_file = f.readlines()\n",
    "with open(f'SemEval2022-Task11_Train-Dev/ES-Spanish/es_train_lwtr.conll') as f:\n",
    "    lwtr_file = f.readlines()\n",
    "with open(f'SemEval2022-Task11_Train-Dev/ES-Spanish/es_train_mr.conll') as f:\n",
    "    mr_file = f.readlines()\n",
    "with open(f'SemEval2022-Task11_Train-Dev/ES-Spanish/es_train_sr.conll') as f:\n",
    "    sr_file = f.readlines()\n",
    "with open(f'SemEval2022-Task11_Train-Dev/ES-Spanish/es_train_sis.conll') as f:\n",
    "    sis_file = f.readlines()\n",
    "\n",
    "combined_file = og_file + lwtr_file + mr_file + sr_file + sis_file\n",
    "\n",
    "with open('SemEval2022-Task11_Train-Dev/ES-Spanish/es_train_combined.conll', 'w') as f:\n",
    "    for line in combined_file:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-imperial",
   "metadata": {},
   "source": [
    "Fix SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "global-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/EN-English/en_train_combined.conll') as f:\n",
    "    file = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ready-milan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u202f _ _ O'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '  _ _ O'\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "expired-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in file:\n",
    "    if '\\xa0' in line.split(' _ _ ')[0] or ' ' in line.split(' _ _ ')[0] or '\\u3000' in line.split(' _ _ ')[0] or '\\xa0—' in line.split(' _ _ ')[0] or '—\\xa0' in line.split(' _ _ ')[0]:\n",
    "        print(line)\n",
    "    elif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "convertible-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, line in enumerate(file):\n",
    "    if '\\xa0—' in line:\n",
    "        file[index] = line.replace('\\xa0—', '')\n",
    "    elif '—\\xa0' in line:\n",
    "        file[index] = line.replace('—\\xa0', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "agreed-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/EN-English/en_train_combined.conll', 'w') as f:\n",
    "    for i in file:\n",
    "        f.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infinite-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for  space + - \n",
    "for index, line in enumerate(file):\n",
    "    if '\\xa0— ' in line:\n",
    "        temp = line.split(' _ _ ')\n",
    "        temp[0] = temp[0][:-2]\n",
    "        file[index] = ' _ _ '.join(temp)\n",
    "    elif '—\\xa0' in line:\n",
    "        temp = line.split(' _ _ ')\n",
    "        temp[0] = temp[0][2:]\n",
    "        file[index] = ' _ _ '.join(temp)\n",
    "    elif '\\xa0' in line.split(' _ _ ')[0]:\n",
    "        file[index] = line.replace(\"\\xa0\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reliable-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import unicodedata\n",
    "\n",
    "for index, line in enumerate(file):\n",
    "    if line.strip() and not line.startswith(\"# id \"):\n",
    "        #line = unicodedata.normalize(\"NFKD\", line.split(' _ _ '))\n",
    "        if '\\xa0' in line.split(' _ _ ')[0] or ' ' in line.split(' _ _ ')[0] or '\\u3000' in line.split(' _ _ ')[0]:\n",
    "            #print('inside if: ', line)\n",
    "            temp = line.split(' _ _ ')\n",
    "            #print('temp: ', temp)\n",
    "            temp[0] = temp[0].split()[0]\n",
    "            #print('new temp: ', temp)\n",
    "            file[index] = ' _ _ '.join(temp)\n",
    "            #print('a[index]:', a[index])\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "complicated-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2022-Task11_Train-Dev/DE-German/de_train_all.conll', 'w') as f:\n",
    "    for i in file:\n",
    "        f.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "opposed-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['records ile _ _ I-CORP\\n',\n",
    "     'slkfd _ _ O\\n',\n",
    "     '# id sdf',\n",
    "     '\\n',\n",
    "     'and the _ _ B-GRP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "elect-addition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–\\xa0darunter _ _ O'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = '– darunter _ _ O'\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "arabic-translation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "a = ['日本代表　試合別出場記録 _ _ O\\n', 'slkfd _ _ O\\n']\n",
    "for index, line in a:\n",
    "    if '\\u3000' in line.split(' _ _ ')[0]:\n",
    "        temp = line\n",
    "        a[index] = line.split(' _ _ ')[0].split('\\u300')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "private-simon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'日本代表'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split(' _ _ ')[0].split('\\u3000')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-adventure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERBaseAnnotator(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 train_data=None,\n",
    "                 dev_data=None,\n",
    "                 lr=1e-5,\n",
    "                 dropout_rate=0.1,\n",
    "                 batch_size=16,\n",
    "                 tag_to_id=None,\n",
    "                 stage='fit',\n",
    "                 pad_token_id=1,\n",
    "                 encoder_model='xlm-roberta-large',\n",
    "                 num_gpus=1):\n",
    "        super(NERBaseAnnotator, self).__init__()\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.dev_data = dev_data\n",
    "\n",
    "        self.id_to_tag = {v: k for k, v in tag_to_id.items()}\n",
    "        self.tag_to_id = tag_to_id\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.stage = stage\n",
    "        self.num_gpus = num_gpus\n",
    "        self.target_size = len(self.id_to_tag)\n",
    "\n",
    "        # set the default baseline model here\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        self.encoder_model = encoder_model\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_model, return_dict=True)\n",
    "\n",
    "        self.feedforward = nn.Linear(in_features=self.encoder.config.hidden_size, out_features=self.target_size)\n",
    "        self.crf_layer = ConditionalRandomField(num_tags=self.target_size, constraints=allowed_transitions(constraint_type=\"BIO\", labels=self.id_to_tag))\n",
    "\n",
    "        self.lr = lr\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.span_f1 = SpanF1()\n",
    "        self.setup_model(self.stage)\n",
    "        self.save_hyperparameters('pad_token_id', 'encoder_model')\n",
    "\n",
    "    def setup_model(self, stage_name):\n",
    "        if stage_name == 'fit' and self.train_data is not None:\n",
    "            # Calculate total steps\n",
    "            train_batches = len(self.train_data) // (self.batch_size * self.num_gpus)\n",
    "            self.total_steps = 50 * train_batches\n",
    "\n",
    "            self.warmup_steps = int(self.total_steps * 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-indiana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-uncle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cultural-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blah(number):\n",
    "    return Example(number)\n",
    "\n",
    "class Example(object, number):\n",
    "    def __init__(self):\n",
    "        self.itsProblem = \"problem\"\n",
    "\n",
    "    def the_example(self, loss):\n",
    "        loss = \"5\"\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "breathing-brown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Example.the_example of <__main__.Example object at 0x00000205D2766880>>\n"
     ]
    }
   ],
   "source": [
    "a = Example(5)\n",
    "print(a.the_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "soviet-hudson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.the_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from utils.utils import get_reader, train_model, create_model, save_model, parse_args, get_tagset\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    timestamp = time.time()\n",
    "    sg = parse_args()\n",
    "    out_dir_path = sg.out_dir + '/' + sg.model_name\n",
    "\n",
    "    # load the dataset first\n",
    "    train_data = get_reader(file_path=sg.train, target_vocab=get_tagset(sg.iob_tagging), encoder_model=sg.encoder_model, max_instances=sg.max_instances, max_length=sg.max_length)\n",
    "    dev_data = get_reader(file_path=sg.dev, target_vocab=get_tagset(sg.iob_tagging), encoder_model=sg.encoder_model, max_instances=sg.max_instances, max_length=sg.max_length)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    #epochs = 10, dropout [0.5, 0.6, 0.7, 0.8], batch_size [32, 64], learning_rate [0.01, 0.001, 0.0001]\n",
    "    epochs = [2, 10]\n",
    "    learning_rate = [0.01, 0.0001]\n",
    "    dropout = [0.5, 0.8]\n",
    "    batch_size = [32, 64]\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        for lr in learning_rate\n",
    "            for drop in dropout:\n",
    "                for batch in batch_size:\n",
    "                    model = create_model(train_data=train_data, dev_data=dev_data, tag_to_id=train_data.get_target_vocab(),\n",
    "                                         dropout_rate=drop, batch_size=batch, stage=sg.stage, lr=lr,\n",
    "                                         encoder_model=sg.encoder_model, num_gpus=sg.gpus)\n",
    "\n",
    "                    trainer = train_model(model=model, out_dir=out_dir_path, epochs=epochs)\n",
    "\n",
    "                    # use pytorch lightnings saver here.\n",
    "                    out_model_path = save_model(trainer=trainer, out_dir=out_dir_path, model_name=f'{sg.model_name}_e{epoch}_lr{lr}_d{drop}_bs{batch}', timestamp=timestamp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "extended-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'ko_train'\n",
    "b = str(10)\n",
    "c = a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "interim-harvest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ko_train10'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-tunnel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
